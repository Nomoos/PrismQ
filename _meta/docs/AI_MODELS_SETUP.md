# AI Models Setup Guide

**Document Type**: Setup Guide  
**Scope**: Project-wide  
**Last Updated**: 2025-12-05

## Overview

PrismQ uses local LLM models through Ollama for AI-powered content generation and SEO metadata optimization. This guide covers how to set up Ollama and configure AI models for use with PrismQ.

## Prerequisites

- Windows, macOS, or Linux operating system
- At least 16GB RAM (32GB recommended for larger models)
- GPU with sufficient VRAM for model inference (optional but recommended)
- Stable internet connection for model downloads

## Step 1 â€“ Install Ollama

Download and install Ollama from the official website:

**Download Link**: https://ollama.com/download

### Windows Installation
1. Download the Windows installer from the link above
2. Run the installer and follow the prompts
3. Ollama will be available as a system service

### macOS Installation
1. Download the macOS app from the link above
2. Drag Ollama to your Applications folder
3. Launch Ollama from Applications

### Linux Installation
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## Step 2 â€“ Pull the Qwen2.5-14B Model

Ollama has an official Qwen2.5 entry in its library, including a 14B variant. This is the recommended model for PrismQ content generation tasks.

In your terminal, run:

```bash
ollama pull qwen2.5:14b
```

This will download approximately 9GB of model weights. The download time depends on your internet connection speed.

### Alternative Qwen2.5 Models

Depending on your hardware capabilities, you can use different Qwen2.5 variants:

| Model | Size | VRAM Required | Use Case |
|-------|------|---------------|----------|
| `qwen2.5:7b` | ~4.5GB | 8GB | Lighter weight, faster inference |
| `qwen2.5:14b` | ~9GB | 16GB | **Recommended** - Best balance of quality and speed |
| `qwen2.5:32b` | ~20GB | 24GB+ | Highest quality, requires high-end GPU |

### Other Supported Models

PrismQ also supports other Ollama models. For SEO-specific tasks, the Keywords module uses:

| Model | Size | VRAM Required | Use Case |
|-------|------|---------------|----------|
| `llama3.1:70b-q4_K_M` | ~40GB | 48GB+ | SEO metadata generation (Keywords module default) |

To pull this model:
```bash
ollama pull llama3.1:70b-q4_K_M
```

## Model Comparison for High-End Systems (RTX 5090)

For users with an NVIDIA RTX 5090 (32GB VRAM), you have access to the most powerful local AI models. Here's a comprehensive comparison for story writing and content generation:

### Recommended Models for RTX 5090

| Model | Parameters | VRAM Usage | Story Quality | Speed | Best For |
|-------|------------|------------|---------------|-------|----------|
| **Qwen2.5:32b** | 32B | ~20GB | â­â­â­â­â­ | Medium | **Best overall for creative writing** |
| **Qwen2.5:14b** | 14B | ~9GB | â­â­â­â­ | Fast | Balanced quality and speed |
| **Llama3.1:70b-q4** | 70B | ~40GB | â­â­â­â­â­ | Slow | Highest quality, requires quantization |
| **Llama3.3:70b** | 70B | ~40GB | â­â­â­â­â­ | Slow | Latest Llama, improved reasoning |
| **Mistral-Large** | 123B | ~32GB | â­â­â­â­â­ | Slow | Complex narratives |
| **DeepSeek-V2** | 236B | ~32GB | â­â­â­â­ | Medium | Long-form content |

### Llama 3.1 405B vs Llama 3.3 70B

| Aspect | Llama 3.1 â€” 405B | Llama 3.3 â€” 70B |
|--------|------------------|-----------------|
| **Parameters** | 405B | 70B |
| **VRAM Required** | ~200GB+ (requires multi-GPU or cloud) | ~40GB (Q4 quantized) |
| **Context Length** | 128K tokens | 128K tokens |
| **Quality** | â­â­â­â­â­ (best-in-class) | â­â­â­â­â­ (excellent) |
| **Speed** | Very Slow | Medium |
| **Local RTX 5090** | âŒ Too large | âœ… With quantization |
| **Ollama Support** | âŒ Cloud/API only | âœ… `ollama pull llama3.3:70b` |

### Understanding Quantization

**Co je kvantizace?** Kvantizace je technika komprese modelu, kterÃ¡ sniÅ¾uje pÅ™esnost vah (napÅ™. z 16-bit na 4-bit). To vÃ½raznÄ› zmenÅ¡Ã­ velikost modelu a VRAM poÅ¾adavky.

#### KompletnÃ­ pÅ™ehled kvantizaÄnÃ­ch variant

| Kvantizace | Kvalita vs OriginÃ¡l | VRAM (70B model) | Rychlost | DoporuÄenÃ­ |
|------------|---------------------|------------------|----------|------------|
| **FP16** (bez kvantizace) | 100% | ~140GB | NejpomalejÅ¡Ã­ | âŒ PÅ™Ã­liÅ¡ velkÃ© pro RTX 5090 |
| **Q8_0** (8-bit) | ~99.5% | ~70GB | PomalÃ¡ | âŒ PÅ™Ã­liÅ¡ velkÃ© pro RTX 5090 |
| **Q6_K** (6-bit) | ~98.5% | ~54GB | StÅ™ednÃ­ | âš ï¸ Na hranici, mÅ¯Å¾e fungovat s offloadingem |
| **Q5_K_M** (5-bit) | ~97% | ~47GB | RychlÃ¡ | âœ… DobrÃ¡ volba pro kvalitu |
| **Q5_K_S** (5-bit small) | ~96% | ~45GB | RychlÃ¡ | âœ… DobrÃ¡ alternativa |
| **Q4_K_M** (4-bit medium) | ~95% | ~40GB | Velmi rychlÃ¡ | âœ… **DOPORUÄŒENO pro RTX 5090** |
| **Q4_K_S** (4-bit small) | ~94% | ~38GB | Velmi rychlÃ¡ | âœ… NejrychlejÅ¡Ã­ kvalitnÃ­ varianta |
| **Q3_K_M** (3-bit) | ~90% | ~33GB | ExtrÃ©mnÄ› rychlÃ¡ | âš ï¸ ZnatelnÃ¡ ztrÃ¡ta kvality |
| **IQ4_XS** (4-bit i-quant) | ~94.5% | ~36GB | Velmi rychlÃ¡ | âœ… ModernÃ­ alternativa k Q4 |

#### ğŸ† FinÃ¡lnÃ­ doporuÄenÃ­ pro RTX 5090 (32GB VRAM)

**Pro maximÃ¡lnÃ­ kvalitu:** `Q4_K_M` nebo `Q5_K_S`
- Q4_K_M nabÃ­zÃ­ nejlepÅ¡Ã­ pomÄ›r kvalita/VRAM pro 32GB karty
- RozdÃ­l mezi Q4_K_M a Q6_K je v praxi tÃ©mÄ›Å™ nepostÅ™ehnutelnÃ½ pro kreativnÃ­ psanÃ­
- Q6_K je pÅ™Ã­liÅ¡ velkÃ½ pro RTX 5090 bez CPU offloadingu

```bash
# ğŸ† NEJLEPÅ Ã VOLBA pro RTX 5090 32GB - Llama 3.3 70B Q4_K_M
ollama pull llama3.3:70b-q4_K_M

# Alternativa pro o nÄ›co vyÅ¡Å¡Ã­ kvalitu (pomalejÅ¡Ã­)
ollama pull llama3.3:70b-q5_K_S

# Pro Qwen2.5 32B (vejde se celÃ½ bez kvantizace)
ollama pull qwen2.5:32b
```

> **PoznÃ¡mka ke Q6_K:** I kdyÅ¾ Q6_K nabÃ­zÃ­ ~98.5% kvality, vyÅ¾aduje ~54GB VRAM pro 70B model. Na RTX 5090 (32GB) by musel pouÅ¾Ã­t CPU offloading, coÅ¾ dramaticky zpomalÃ­ inference. Pro vaÅ¡i sestavu doporuÄuji Q4_K_M - ztrÃ¡ta kvality je minimÃ¡lnÃ­ (~5%) a rychlost bude vÃ½raznÄ› lepÅ¡Ã­.

### Model Recommendations by PrismQ Task

| Task | Best Model | Alternative | Why |
|------|------------|-------------|-----|
| **Idea Generation** | Llama 3.3:70b | Qwen2.5:32b | Strong creative reasoning, diverse ideas |
| **Title Creation** | Qwen2.5:14b | Llama 3.3:70b | Fast, concise outputs, good for iteration |
| **Script Writing** | Qwen2.5:32b | Llama 3.1:70b-q4 | Best narrative quality, instruction following |
| **Review/Editing** | Llama 3.3:70b | Llama 3.1:70b-q4 | Superior analytical and reasoning capabilities |
| **SEO Metadata** | Llama 3.1:70b-q4 | Qwen2.5:14b | Consistent, structured outputs |

#### Task-Specific Configuration:

```python
from T.Publishing.SEO.Keywords import AIConfig

# Idea Generation - creative, diverse
idea_config = AIConfig(
    model="llama3.3:70b",
    temperature=0.8,  # Higher for creativity
    enable_ai=True
)

# Title Creation - fast iteration
title_config = AIConfig(
    model="qwen2.5:14b",
    temperature=0.5,
    enable_ai=True
)

# Script Writing - high quality narrative
script_config = AIConfig(
    model="qwen2.5:32b",
    temperature=0.7,
    max_tokens=4000,
    enable_ai=True
)

# Review/Editing - analytical
review_config = AIConfig(
    model="llama3.3:70b",
    temperature=0.3,  # Lower for consistent analysis
    enable_ai=True
)
```

#### When to Use Llama 3.1 405B:
- Cloud/API access (Together AI, Fireworks, Groq)
- Maximum quality requirements
- Complex reasoning tasks
- Enterprise production workflows

#### When to Use Llama 3.3 70B:
- **Recommended for local RTX 5090** âœ…
- Best balance of quality and local inference
- Reviews and analytical tasks
- Idea generation with strong reasoning

```bash
# Install Llama 3.3 70B for local use
ollama pull llama3.3:70b-q4_K_M
```

### MPT-7B-StoryWriter (HuggingFace)

The [MPT-7B-StoryWriter](https://huggingface.co/mosaicml/mpt-7b-storywriter) is a specialized model fine-tuned specifically for story writing with an impressive 65K context length.

| Aspect | MPT-7B-StoryWriter | Qwen2.5:32b |
|--------|-------------------|-------------|
| **Parameters** | 7B | 32B |
| **VRAM Required** | ~8GB | ~20GB |
| **Context Length** | 65,536 tokens | 32,768 tokens |
| **Story Quality** | â­â­â­â­ (specialized) | â­â­â­â­â­ (general) |
| **Inference Speed** | Very Fast | Medium |
| **Ollama Support** | âŒ (requires manual setup) | âœ… Native |
| **Best Use Case** | Long-form novels, extended narratives | All creative content |

#### When to Choose MPT-7B-StoryWriter:
- Writing very long stories (novels, extended series)
- Need 65K context for maintaining consistency
- Prefer faster inference over raw quality
- Working with HuggingFace Transformers directly

#### When to Choose Qwen2.5:32b:
- General creative writing (stories, scripts, dialogue)
- Need better instruction following
- Prefer easy Ollama integration
- Want highest quality output

### ğŸ“š Modely optimalizovanÃ© pro tvorbu pÅ™Ã­bÄ›hÅ¯

Pro kreativnÃ­ psanÃ­ a tvorbu pÅ™Ã­bÄ›hÅ¯ existujÃ­ specializovanÃ© modely s lepÅ¡Ã­m vÃ½konem neÅ¾ obecnÃ© LLM:

#### ğŸ§ª ReÃ¡lnÃ© testy modelÅ¯ (English YA Fiction)

Na zÃ¡kladÄ› internÃ­ho testovÃ¡nÃ­ PrismQ s anglickÃ½mi pÅ™Ã­bÄ›hy pro US/CA publikum:

| Model | Test pÅ™Ã­bÄ›h | SkÃ³re | Struktura | Dialog | Postavy | NapÄ›tÃ­ | Konzistence | ÄŒitelnost | PoznÃ¡mka |
|-------|-------------|-------|-----------|--------|---------|--------|-------------|-----------|----------|
| **qwen2.5:32b (EN)** | The Lighthouse Keeper's Secret | **7.8/10** | 8 | 9 | 6.5 | 8 | 7 | 7.5 | 7 | ğŸ† PÅ™ekvapivÄ› ÄistÃ©, soudrÅ¾nÃ©, ÄtivÃ© â€” mnohem lepÅ¡Ã­ neÅ¾ CZ |
| **mistral-nemo:12b (EN)** | The Whispering Grove | **7.5/10** | 8 | 9 | 6 | 7.5 | 6 | 7 | 6 | AngliÄtina vÃ½raznÄ› zvedÃ¡ kvalitu, dobrÃ© YA-fantasy |

> **KlÃ­ÄovÃ© zjiÅ¡tÄ›nÃ­:** AnglickÃ¡ verze vÃ½raznÄ› pÅ™evyÅ¡uje Äeskou u obou modelÅ¯. Pro US/CA publikum doporuÄujeme vÅ¾dy generovat v angliÄtinÄ›.

#### ğŸ† FinÃ¡lnÃ­ doporuÄenÃ­ pro English YA Fiction

Na zÃ¡kladÄ› testÅ¯ doporuÄujeme pro **americkÃ©/kanadskÃ© teen publikum**:

| Priorita | Model | SkÃ³re | NejlepÅ¡Ã­ pro |
|----------|-------|-------|--------------|
| **#1** | **Qwen2.5:32b** | 7.8/10 | Family Drama, Teen Drama, Mystery |
| **#2** | **Mistral-Nemo:12b** | 7.5/10 | YA Fantasy, Slice of Life, Reddit Stories |

```bash
# PrimÃ¡rnÃ­ model pro EN YA content
ollama pull qwen2.5:32b

# SekundÃ¡rnÃ­/rychlejÅ¡Ã­ model
ollama pull mistral-nemo:12b
```

#### ğŸ” SrovnÃ¡nÃ­ 32B modelÅ¯ pro kreativnÃ­ psanÃ­

| Model | Fine-tuning | Kvalita prÃ³zy | Kontext | AngliÄtina | Ollama | Benchmarks |
|-------|-------------|---------------|---------|------------|--------|------------|
| **Qwen2.5:32b-Instruct** | General | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 83.5 |
| **Yi-1.5-34B-Chat** | Chat/Creative | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 81.2 |
| **DeepSeek-V2-Lite (27B)** | General | â­â­â­â­ | 128K | DobrÃ¡ | âœ… | MMLU: 79.8 |
| **Mixtral-8x7B (47B MoE)** | Instruct | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 81.1 |
| **Command-R (35B)** | RAG/Chat | â­â­â­â­ | 128K | VÃ½bornÃ¡ | âœ… | MMLU: 78.5 |

#### ğŸ“– Fine-tuned modely pro kreativnÃ­ psanÃ­ (32B tÅ™Ã­da)

| Model | Specializace | Kvalita | VRAM | Zdroj |
|-------|--------------|---------|------|-------|
| **Nous-Hermes-2-Yi-34B** | Creative writing, RP | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Airoboros-34B** | Creative, storytelling | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Dolphin-2.6-Yi-34B** | Uncensored creative | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Goliath-120B** (merged) | Premium creative | â­â­â­â­â­ | ~70GB | HuggingFace |
| **Chronos-Hermes-34B** | Long-form fiction | â­â­â­â­â­ | ~22GB | HuggingFace |
| **StellarBright-Qwen2.5-32B** | Creative writing | â­â­â­â­â­ | ~20GB | HuggingFace |

> **PoznÃ¡mka:** Fine-tuned modely pro kreativnÃ­ psanÃ­ Äasto pÅ™ekonÃ¡vajÃ­ vÄ›tÅ¡Ã­ obecnÃ© modely v kvalitÄ› narativu.

#### ğŸ¯ DoporuÄenÃ­ pro cÃ­lovÃ© publikum: Teen/Young Adult (10-20, US Å¾eny)

Pro americkÃ© a kanadskÃ© anglicky mluvÃ­cÃ­ publikum (pÅ™edevÅ¡Ã­m mladÃ© Å¾eny 10-20 let):

| Å½Ã¡nr | ğŸ† DoporuÄenÃ½ model | Alternativa | ProÄ |
|------|---------------------|-------------|------|
| **Reddit Stories** | Nous-Hermes-2-Yi-34B | Qwen2.5:32b | AutentickÃ½ Reddit styl, relatable |
| **Family Drama** | Qwen2.5:32b | Airoboros-34B | EmocionÃ¡lnÃ­ hloubka, realistickÃ© dialogy |
| **Teen Drama** | Dolphin-2.6-Yi-34B | Nous-Hermes-2-Yi-34B | Teen hlas, modernÃ­ slang |
| **Teen Stories** | Nous-Hermes-2-Yi-34B | Mistral-Nemo | YA narativ, engagement |
| **Romance (YA)** | Chronos-Hermes-34B | Qwen2.5:32b | EmotivnÃ­, clean romance |
| **Thriller/Mystery** | Qwen2.5:32b | Command-R | NapÄ›tÃ­, twist endings |
| **AITA/Confession** | Nous-Hermes-2-Yi-34B | Dolphin-2.6-Yi-34B | AutentickÃ½ POV |
| **Slice of Life** | Mistral-Nemo | Qwen2.5:32b | KaÅ¾dodennÃ­ situace, relatability |

#### ğŸ† TOP 3 pro Teen/YA obsah na RTX 5090

**1. Nous-Hermes-2-Yi-34B** - NejlepÅ¡Ã­ pro Reddit/Teen stories
```bash
ollama pull nous-hermes2:yi-34b-q4_K_M
```
- SpecializovanÃ½ na creative writing a roleplay
- PÅ™irozenÃ½ teen dialog a POV
- VÃ½bornÃ½ pro AITA, relationship drama, confession stories
- ~22GB VRAM (Q4)

**2. Qwen2.5:32b** - UniverzÃ¡lnÃ­ vysokÃ¡ kvalita
```bash
ollama pull qwen2.5:32b
```
- NejlepÅ¡Ã­ balance kvality a rychlosti
- VÃ½bornÃ¡ angliÄtina, emotivnÃ­ prÃ³za
- IdeÃ¡lnÃ­ pro family drama, romance
- ~20GB VRAM

**3. Dolphin-2.6-Yi-34B** - Pro autentickÃ½ teen hlas
```bash
ollama pull dolphin2.6:yi-34b-q4_K_M
```
- Uncensored, pÅ™irozenÃ© dialogy
- ModernÃ­ slang a teen expressions
- VhodnÃ½ pro edgier teen drama
- ~22GB VRAM (Q4)

#### ğŸ“Š Statistiky a benchmarky 32B modelÅ¯ (creative writing)

Na zÃ¡kladÄ› komunitnÃ­ch testÅ¯ a r/LocalLLaMA:

| Model | Reddit Stories | Dialogy | Emotivnost | Konzistence | CelkovÄ› |
|-------|----------------|---------|------------|-------------|---------|
| Nous-Hermes-2-Yi-34B | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | **#1** |
| Qwen2.5:32b | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | **#2** |
| Airoboros-34B | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | **#3** |
| Yi-34B-Chat | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | **#4** |

#### Konfigurace pro Teen/YA content

```python
from T.Publishing.SEO.Keywords import AIConfig

# Reddit Stories / AITA style
reddit_config = AIConfig(
    model="nous-hermes2:yi-34b-q4_K_M",
    temperature=0.85,           # VyÅ¡Å¡Ã­ pro autenticitu
    max_tokens=3000,
    enable_ai=True
)

# Teen Drama / Family Drama
drama_config = AIConfig(
    model="qwen2.5:32b",
    temperature=0.75,
    max_tokens=4000,
    enable_ai=True
)

# Teen Stories (YA fiction)
teen_stories_config = AIConfig(
    model="nous-hermes2:yi-34b-q4_K_M",
    temperature=0.8,
    max_tokens=5000,           # DelÅ¡Ã­ kapitoly
    enable_ai=True
)
```

#### Instalace modelÅ¯ pro Teen/YA content

```bash
# KompletnÃ­ sada pro Teen/YA publikum na RTX 5090

# 1. PrimÃ¡rnÃ­ pro Reddit stories a teen drama
ollama pull nous-hermes2:yi-34b-q4_K_M

# 2. UniverzÃ¡lnÃ­ vysokÃ¡ kvalita
ollama pull qwen2.5:32b

# 3. Pro edgier teen content
ollama pull dolphin2.6:yi-34b-q4_K_M

# 4. Pro dlouhÃ© sÃ©rie
ollama pull yi:34b-chat-q4_K_M
```

> **Tip pro US teen publikum:** PouÅ¾Ã­vejte `temperature=0.8-0.9` pro autentiÄtÄ›jÅ¡Ã­ dialogy. Teen content vyÅ¾aduje souÄasnÃ½ slang, pop culture reference a relatable situace.

#### SrovnÃ¡nÃ­ modelÅ¯ pro psanÃ­ pÅ™Ã­bÄ›hÅ¯

| Model | Parametry | VRAM | Kvalita pÅ™Ã­bÄ›hÅ¯ | Kontext | Ollama | DoporuÄenÃ­ |
|-------|-----------|------|-----------------|---------|--------|------------|
| **Mistral-Nemo-Instruct-2407** | 12B | ~8GB | â­â­â­â­â­ | 128K | âœ… | ğŸ† **NEJLEPÅ Ã pro pÅ™Ã­bÄ›hy** |
| **Qwen2.5:32b** | 32B | ~20GB | â­â­â­â­â­ | 32K | âœ… | VÃ½bornÃ¡ kvalita, versatilnÃ­ |
| **Llama-3.1-Storm-8B** | 8B | ~6GB | â­â­â­â­ | 8K | âœ… | KreativnÃ­, rychlÃ½ |
| **Nous-Hermes-2-Mixtral** | 47B | ~28GB | â­â­â­â­â­ | 32K | âœ… | NejlepÅ¡Ã­ MoE pro pÅ™Ã­bÄ›hy |
| **Yi-34B-Chat** | 34B | ~22GB | â­â­â­â­â­ | 200K | âœ… | ExtrÃ©mnÃ­ kontext |
| **DeepSeek-Coder-V2-Lite** | 16B | ~10GB | â­â­â­â­ | 128K | âœ… | DobrÃ½ pro dialogy |
| **MPT-7B-StoryWriter** | 7B | ~8GB | â­â­â­â­ | 65K | âŒ | SpecializovanÃ½ na romÃ¡ny |
| **Fimbulvetr-11B** | 11B | ~8GB | â­â­â­â­â­ | 8K | âš ï¸ | VÃ½jimeÄnÃ½ pro RP/fiction |

#### ğŸ† Top 3 doporuÄenÃ© modely pro pÅ™Ã­bÄ›hy na RTX 5090

**1. Mistral-Nemo-Instruct-2407** - NejlepÅ¡Ã­ volba
```bash
ollama pull mistral-nemo:12b
```
- 128K tokenÅ¯ kontextu (perfektnÃ­ pro dlouhÃ© pÅ™Ã­bÄ›hy)
- VÃ½jimeÄnÃ¡ kreativita a koherence
- OptimalizovÃ¡no pro narativnÃ­ Ãºlohy
- Vejde se do 32GB VRAM bez kvantizace

**2. Nous-Hermes-2-Mixtral-8x7B** - Premium kvalita
```bash
ollama pull nous-hermes2-mixtral:8x7b-q4_K_M
```
- MoE architektura (efektivnÃ­ vyuÅ¾itÃ­ parametrÅ¯)
- Å piÄkovÃ¡ kvalita prÃ³zy
- VyÅ¾aduje ~28GB VRAM

**3. Yi-34B-Chat** - Pro extrÃ©mnÄ› dlouhÃ© pÅ™Ã­bÄ›hy
```bash
ollama pull yi:34b-chat-q4_K_M
```
- 200K tokenÅ¯ kontextu (nejdelÅ¡Ã­)
- IdeÃ¡lnÃ­ pro romÃ¡ny a sÃ©rie
- VyÅ¾aduje ~22GB VRAM (Q4)

#### SpecializovanÃ© modely pro rÅ¯znÃ© Å¾Ã¡nry

| Å½Ã¡nr | DoporuÄenÃ½ model | Alternativa |
|------|------------------|-------------|
| **Horror/Dark Fantasy** | Mistral-Nemo | Fimbulvetr-11B |
| **Romantika** | Qwen2.5:32b | Yi-34B-Chat |
| **Sci-Fi** | Nous-Hermes-2-Mixtral | DeepSeek-V2 |
| **Detektivky** | Llama 3.3:70b | Mistral-Nemo |
| **DÄ›tskÃ© pÅ™Ã­bÄ›hy** | Qwen2.5:14b | Mistral-Nemo |
| **EpickÃ¡ fantasy** | Yi-34B-Chat | Nous-Hermes-2 |
| **KrÃ¡tkÃ© povÃ­dky** | Mistral-Nemo | Qwen2.5:32b |
| **Dialogy/ScÃ©nÃ¡Å™e** | Llama 3.3:70b | Qwen2.5:32b |

#### Instalace nejlepÅ¡Ã­ch modelÅ¯ pro pÅ™Ã­bÄ›hy

```bash
# KompletnÃ­ sada pro profesionÃ¡lnÃ­ tvorbu pÅ™Ã­bÄ›hÅ¯ na RTX 5090

# 1. PrimÃ¡rnÃ­ model pro pÅ™Ã­bÄ›hy (doporuÄeno)
ollama pull mistral-nemo:12b

# 2. Pro dlouhÃ© romÃ¡ny
ollama pull yi:34b-chat-q4_K_M

# 3. Pro premium kvalitu prÃ³zy
ollama pull nous-hermes2-mixtral:8x7b-q4_K_M

# 4. UniverzÃ¡lnÃ­ zÃ¡loha
ollama pull qwen2.5:32b
```

#### Konfigurace pro tvorbu pÅ™Ã­bÄ›hÅ¯

```python
from T.Publishing.SEO.Keywords import AIConfig

# OptimÃ¡lnÃ­ konfigurace pro kreativnÃ­ psanÃ­ pÅ™Ã­bÄ›hÅ¯
story_writing_config = AIConfig(
    model="mistral-nemo:12b",  # NejlepÅ¡Ã­ pro pÅ™Ã­bÄ›hy
    api_base="http://localhost:11434",
    temperature=0.8,           # VyÅ¡Å¡Ã­ pro kreativitu
    max_tokens=4000,           # DlouhÃ© kapitoly
    enable_ai=True
)

# Pro velmi dlouhÃ© pÅ™Ã­bÄ›hy (romÃ¡ny)
novel_config = AIConfig(
    model="yi:34b-chat-q4_K_M",
    temperature=0.7,
    max_tokens=8000,           # CelÃ© kapitoly
    enable_ai=True
)

# Pro dialogy a scÃ©nÃ¡Å™e
dialogue_config = AIConfig(
    model="llama3.3:70b-q4_K_M",
    temperature=0.6,           # KonzistentnÄ›jÅ¡Ã­ dialogy
    max_tokens=2000,
    enable_ai=True
)
```

> **Tip pro tvorbu pÅ™Ã­bÄ›hÅ¯:** PouÅ¾Ã­vejte vyÅ¡Å¡Ã­ `temperature` (0.7-0.9) pro kreativnÄ›jÅ¡Ã­ vÃ½stup. Pro konzistentnÃ­ postavy a zÃ¡pletky udrÅ¾ujte kontext a pouÅ¾Ã­vejte modely s dlouhÃ½m kontextovÃ½m oknem (Yi-34B, Mistral-Nemo).

### ğŸ“– Moving Window Text Generation (ProfesionÃ¡lnÃ­ pipeline pro romÃ¡ny)

OsvÄ›dÄenÃ¡ metoda pro psanÃ­ dlouhÃ½ch textÅ¯ (romÃ¡ny, fanfikce, sÃ©rie) bez pÃ¡du kvality.

#### ğŸ¯ ZÃ¡kladnÃ­ princip

Model **nikdy negeneruje celÃ½ text najednou**. MÃ­sto toho generuje "bloky" (250â€“600 slov) a po kaÅ¾dÃ©m bloku:
1. ShrnutÃ­ (summary)
2. Extrakce klÃ­ÄovÃ½ch faktÅ¯
3. PlÃ¡n dalÅ¡Ã­ho dÄ›je
4. Re-injekce postav a tÃ³nu

#### â­ ProfesionÃ¡lnÃ­ Pipeline (6 krokÅ¯)

**ğŸ”¹ Krok 1 â€” Outline (kostra pÅ™Ã­bÄ›hu)**

```
Write a detailed story outline of 10â€“18 beats.
Include:
- setting
- main character arc
- emotional beats
- conflict escalation
- climax
- resolution
Keep it high-level.
```
> PouÅ¾ij nejsilnÄ›jÅ¡Ã­ model (GPT-5.1 / Claude Sonnet)

**ğŸ”¹ Krok 2 â€” Story Bible (pravidla pÅ™Ã­bÄ›hu)**

ZÅ¯stÃ¡vÃ¡ v kontextu celou dobu:
- JmÃ©na, motivace postav
- JazykovÃ© preference
- Tone guide
- ZakÃ¡zanÃ© halucinace
- Pravidla svÄ›ta

**ğŸ”¹ Krok 3 â€” Moving Window Writing**

| Model | Window Size |
|-------|-------------|
| **Qwen 32B** | 1000â€“1500 tokenÅ¯ |
| **Mistral 12B** | 500â€“800 tokenÅ¯ |

**Proces pro kaÅ¾dÃ½ blok:**

```
1ï¸âƒ£ GENERATE BLOCK (300â€“500 slov)
   â†’ Model napÃ­Å¡e dalÅ¡Ã­ segment pÅ™Ã­bÄ›hu

2ï¸âƒ£ SUMMARIZE BLOCK (150â€“250 slov)
   â†’ ShrnutÃ­ slouÅ¾Ã­ jako kondenzovanÃ¡ pamÄ›Å¥

3ï¸âƒ£ EXTRACT FACTS ("story memory")
   Facts so far:
   - Clara moved to Willow Creek to escape city life.
   - Found Evelyn's letters in attic.
   - House shows supernatural behavior.
   - Primary emotional tone: melancholy + suspense.

4ï¸âƒ£ DIRECTIVE (nÃ¡vod na dalÅ¡Ã­ blok)
   Next segment should:
   - escalate tension
   - introduce secondary character
   - foreshadow climax
   - avoid info dumps
   - keep consistent voice

5ï¸âƒ£ REINJEKCE
   Do promptu dÃ¡Å¡:
   - Story bible
   - ShrnutÃ­ + fakta poslednÃ­ch blokÅ¯
   - Directive
   - UkÃ¡zka stylu
   â†’ "Now continue the story."

6ï¸âƒ£ OPAKUJ
   Dokud nemÃ¡Å¡ desÃ­tky tisÃ­c slov.
```

#### ğŸ“Š VÃ½sledky Moving Window vs Standard Generation

| Metoda | Max dÃ©lka | Kvalita | Memory drift | OpakovÃ¡nÃ­ |
|--------|-----------|---------|--------------|-----------|
| **Standard** | 1500â€“3000 slov | KlesÃ¡ | âš ï¸ VysokÃ½ | âš ï¸ ÄŒastÃ© |
| **Moving Window** | 50 000+ slov | StabilnÃ­ | âœ… Å½Ã¡dnÃ½ | âœ… Å½Ã¡dnÃ© |

**ZlepÅ¡enÃ­ s Moving Window:**
- âœ… Å½Ã¡dnÃ½ pÃ¡d kvality
- âœ… Å½Ã¡dnÃ½ memory drift
- âœ… Å½Ã¡dnÃ© kruhovÃ© opakovÃ¡nÃ­
- âœ… LepÅ¡Ã­ pacing a drama
- âœ… KonzistentnÃ­ voice
- âœ… 2â€“3Ã— vyÅ¡Å¡Ã­ originalita a hloubka

#### ğŸš€ Qwen 3 vs Qwen 2.5

| Aspekt | Qwen 2.5 | Qwen 3 | ZlepÅ¡enÃ­ |
|--------|----------|--------|----------|
| DlouhodobÃ¡ koherence | â­â­â­ | â­â­â­â­â­ | +40% |
| AngliÄtina | â­â­â­â­ | â­â­â­â­â­ | +25% |
| SvÄ›tovÃ¡ pravidla (mÃ©nÄ› halucinacÃ­) | â­â­â­ | â­â­â­â­â­ | +35% |
| DramatickÃ¡ vÃ½stavba | â­â­â­â­ | â­â­â­â­â­ | +30% |
| Lyrika a imagery | â­â­â­ | â­â­â­â­â­ | +40% |
| PrÃ¡ce s emocemi | â­â­â­â­ | â­â­â­â­â­ | +30% |
| PÅ™irozenÃ¡ Å™eÄ | â­â­â­â­ | â­â­â­â­â­ | +25% |

> **Qwen 3 32B** â†’ srovnatelnÃ½ s GPT-3.5 Turbo v angliÄtinÄ›  
> **Qwen 3 72B/110B** â†’ blÃ­zko GPT-4.1 stylu tvorby pÅ™Ã­bÄ›hÅ¯

#### ğŸ§© DoporuÄenÃ© Pipeline pro dlouhÃ© pÅ™Ã­bÄ›hy

**ğŸŸ¢ Level 1 â€” LokÃ¡lnÃ­ (bez cloudu)**
```
Qwen 3 72B â†’ moving window â†’ story bible â†’ external summarizer
```
*VÃ½sledek: KompletnÃ­ romÃ¡n*

**ğŸŸ£ Level 2 â€” Hybrid (nejlepÅ¡Ã­ cena/kvalita)**
```
1. Qwen 2.5-32B EN nebo Mistral 12B: draft
2. GPT-5.1 / Sonnet: review + rewrite
3. Czech translation (if needed)
4. Final polish GPT-5.1
```
*Funguje SKVÄšLE pro PrismQ workflow*

**ğŸ”µ Level 3 â€” NejvyÅ¡Å¡Ã­ kvalita**
```
1. GPT-5.1: outline + story bible
2. Qwen 3 32B: long draft generation
3. GPT-5.1: deep literary rewrite
4. GPT-5.1: final author-level polish
```
*VÃ½sledek: Kvalita profesionÃ¡lnÃ­ povÃ­dkovÃ© tvorby*

#### ğŸ’» PrismQ Moving Window Implementation

```python
from T.Publishing.SEO.Keywords import AIConfig

class MovingWindowGenerator:
    """Moving Window generator pro dlouhÃ© pÅ™Ã­bÄ›hy."""
    
    def __init__(self, model: str = "qwen2.5:32b"):
        self.config = AIConfig(
            model=model,
            api_base="http://localhost:11434",
            temperature=0.75,
            enable_ai=True
        )
        self.story_bible = ""
        self.facts = []
        self.summaries = []
    
    def set_story_bible(self, bible: str):
        """NastavÃ­ Story Bible pro celÃ½ pÅ™Ã­bÄ›h."""
        self.story_bible = bible
    
    def generate_block(self, directive: str, window_size: int = 1200):
        """Generuje jeden blok pÅ™Ã­bÄ›hu."""
        context = self._build_context(directive)
        # Generate with context
        return self._call_model(context, window_size)
    
    def _build_context(self, directive: str) -> str:
        """SestavÃ­ kontext pro generovÃ¡nÃ­."""
        recent_summaries = self.summaries[-3:]  # PoslednÃ­ 3 shrnutÃ­
        recent_facts = self.facts[-10:]  # PoslednÃ­ch 10 faktÅ¯
        
        return f'''
Story Bible:
{self.story_bible}

Recent Summaries:
{chr(10).join(recent_summaries)}

Story Facts:
{chr(10).join(recent_facts)}

Directive:
{directive}

Now continue the story:
'''

# PouÅ¾itÃ­:
generator = MovingWindowGenerator("qwen2.5:32b")
generator.set_story_bible("""
Characters: Clara (28, melancholic), Evelyn (ghost, mysterious)
Setting: Willow Creek, haunted Victorian house
Tone: Gothic, suspenseful, emotional
Rules: No explicit violence, slow burn mystery
""")

# GenerovÃ¡nÃ­ blokÅ¯
block1 = generator.generate_block("Introduce Clara arriving at the house")
# ... summarize, extract facts, continue
```

#### ğŸ“‹ Checklist pro Moving Window

- [ ] Story Bible pÅ™ipraven
- [ ] Outline hotovÃ½ (10â€“18 beats)
- [ ] Window size nastaven podle modelu
- [ ] Summarizer nakonfigurovÃ¡n
- [ ] Fact extractor pÅ™ipraven
- [ ] Directive template vytvoÅ™en

### ğŸ”— Integrace Moving Window do PrismQ Workflow

Moving Window technika se hodÃ­ do **specifickÃ½ch krokÅ¯** PrismQ pipeline:

#### Kde pouÅ¾Ã­t Moving Window v PrismQ.T

| Stage | Moving Window? | DÅ¯vod |
|-------|----------------|-------|
| **PrismQ.T.Idea.Creation** | âŒ Ne | KrÃ¡tkÃ½ vÃ½stup (koncept) |
| **PrismQ.T.Story.From.Idea** | âš ï¸ VolitelnÄ› | Pro detailnÄ›jÅ¡Ã­ Story Bible |
| **PrismQ.T.Title.From.Idea** | âŒ Ne | KrÃ¡tkÃ½ vÃ½stup (titulky) |
| **PrismQ.T.Script.From.Title.Idea** | âœ… **ANO** | ğŸ† **HLAVNÃ USE CASE** |
| **PrismQ.T.Script.From.Title.Review.Script** | âœ… **ANO** | Refinement dlouhÃ©ho scriptu |
| **PrismQ.T.Story.Polish** | âœ… **ANO** | FinÃ¡lnÃ­ polish dlouhÃ©ho textu |
| Review stages | âŒ Ne | AnalytickÃ©, ne generativnÃ­ |

#### ğŸ† PrimÃ¡rnÃ­ integrace: Script Generation

```
PrismQ.T.Script.From.Title.Idea
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“– MOVING WINDOW INTEGRATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. NaÄti Idea + Title jako "Story Bible"            â”‚
â”‚ 2. Vygeneruj Outline (10-18 beats) z Idea           â”‚
â”‚ 3. Pro kaÅ¾dÃ½ beat:                                  â”‚
â”‚    a) Generate Block (300-500 slov)                 â”‚
â”‚    b) Summarize Block                               â”‚
â”‚    c) Extract Facts                                 â”‚
â”‚    d) Prepare Directive pro dalÅ¡Ã­ beat              â”‚
â”‚    e) Reinjekce context + continue                  â”‚
â”‚ 4. SpojenÃ­ blokÅ¯ â†’ Script v1                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
PrismQ.T.Review.Title.By.Script.Idea
```

#### NÃ¡vrh integrace do PrismQ.T.Script

**NovÃ½ modul:** `T/Script/From/Idea/Title/MovingWindow/`

```python
# T/Script/From/Idea/Title/MovingWindow/generator.py

from dataclasses import dataclass
from typing import List, Optional
import ollama

@dataclass
class StoryBeat:
    """Jeden beat (segment) pÅ™Ã­bÄ›hu."""
    number: int
    directive: str
    content: str = ""
    summary: str = ""
    facts: List[str] = None

class PrismQMovingWindowScript:
    """
    Moving Window Script Generator pro PrismQ.T.Script.From.Title.Idea
    
    Integrace do workflow:
    - Input: Idea + Title z pÅ™edchozÃ­ch stages
    - Output: Script v1 (dlouhÃ½ text bez memory drift)
    """
    
    def __init__(
        self, 
        model: str = "qwen2.5:32b",
        window_size: int = 1200,  # tokens
        block_words: int = 400    # target words per block
    ):
        self.model = model
        self.window_size = window_size
        self.block_words = block_words
        self.story_bible = ""
        self.outline: List[StoryBeat] = []
        self.generated_blocks: List[str] = []
        self.summaries: List[str] = []
        self.facts: List[str] = []
    
    def from_idea_and_title(self, idea: dict, title: str) -> str:
        """
        HlavnÃ­ entry point pro PrismQ.T.Script.From.Title.Idea
        
        Args:
            idea: Idea objekt z PrismQ.T.Idea.Creation
            title: Title z PrismQ.T.Title.From.Idea
            
        Returns:
            KompletnÃ­ Script v1
        """
        # 1. Build Story Bible from Idea
        self.story_bible = self._build_story_bible(idea, title)
        
        # 2. Generate Outline (10-18 beats)
        self.outline = self._generate_outline(idea, title)
        
        # 3. Generate each beat using Moving Window
        for beat in self.outline:
            block = self._generate_block(beat)
            self.generated_blocks.append(block)
            
            # Summarize and extract facts
            summary = self._summarize_block(block)
            self.summaries.append(summary)
            
            facts = self._extract_facts(block)
            self.facts.extend(facts)
        
        # 4. Combine into final script
        return self._combine_script()
    
    def _build_story_bible(self, idea: dict, title: str) -> str:
        """VytvoÅ™Ã­ Story Bible z Idea a Title."""
        return f'''
# Story Bible

## Title
{title}

## Core Concept
{idea.get("concept", "")}

## Target Audience
{idea.get("audience", "Teen/YA, 10-20, US women")}

## Tone
{idea.get("tone", "Engaging, emotional, relatable")}

## Characters
{idea.get("characters", "")}

## Setting
{idea.get("setting", "")}

## Rules
- No explicit content
- Consistent voice throughout
- Relatable situations for target audience
- Clear emotional arc
'''
    
    def _generate_outline(self, idea: dict, title: str) -> List[StoryBeat]:
        """Generuje 10-18 beat outline."""
        prompt = f'''Based on this idea and title, create a detailed story outline.

Title: {title}
Concept: {idea.get("concept", "")}

Write exactly 12 beats (story segments) that form a complete narrative arc:
- Beat 1-2: Setup and introduction
- Beat 3-4: Rising action begins
- Beat 5-6: Complications
- Beat 7-8: Midpoint twist
- Beat 9-10: Escalation
- Beat 11: Climax
- Beat 12: Resolution

For each beat, provide a 1-2 sentence directive of what should happen.
'''
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # Parse response into StoryBeat objects
        return self._parse_outline(response["message"]["content"])
    
    def _generate_block(self, beat: StoryBeat) -> str:
        """Generuje jeden blok pomocÃ­ Moving Window."""
        # Build context from recent summaries and facts
        recent_context = self._build_recent_context()
        
        prompt = f'''
{self.story_bible}

## Previous Context
{recent_context}

## Current Directive (Beat {beat.number})
{beat.directive}

## Instructions
Write the next segment of the story (~{self.block_words} words).
- Continue naturally from previous context
- Follow the directive
- Maintain consistent voice and tone
- End at a natural pause point

Now continue the story:
'''
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            options={"num_predict": self.window_size}
        )
        
        return response["message"]["content"]
    
    def _build_recent_context(self) -> str:
        """SestavÃ­ kontext z poslednÃ­ch shrnutÃ­ a faktÅ¯."""
        recent_summaries = self.summaries[-3:] if self.summaries else []
        recent_facts = self.facts[-15:] if self.facts else []
        
        context = ""
        if recent_summaries:
            context += "### Recent Summaries\n"
            context += "\n".join(recent_summaries)
            context += "\n\n"
        
        if recent_facts:
            context += "### Story Facts So Far\n"
            context += "\n".join(f"- {fact}" for fact in recent_facts)
        
        return context if context else "This is the beginning of the story."
    
    def _summarize_block(self, block: str) -> str:
        """VytvoÅ™Ã­ shrnutÃ­ bloku."""
        prompt = f'''Summarize this story segment in 2-3 sentences:

{block}

Summary:'''
        
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            options={"num_predict": 200}
        )
        
        return response["message"]["content"]
    
    def _extract_facts(self, block: str) -> List[str]:
        """Extrahuje klÃ­ÄovÃ¡ fakta z bloku."""
        prompt = f'''Extract 3-5 key facts from this story segment.
Format as simple bullet points.

{block}

Facts:'''
        
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            options={"num_predict": 300}
        )
        
        # Parse bullet points
        lines = response["message"]["content"].strip().split("\n")
        return [line.strip("- ").strip() for line in lines if line.strip()]
    
    def _combine_script(self) -> str:
        """SpojÃ­ vÅ¡echny bloky do finÃ¡lnÃ­ho scriptu."""
        return "\n\n".join(self.generated_blocks)
    
    def _parse_outline(self, outline_text: str) -> List[StoryBeat]:
        """Parsuje outline text do StoryBeat objektÅ¯."""
        beats = []
        lines = outline_text.strip().split("\n")
        beat_num = 0
        
        for line in lines:
            line = line.strip()
            if line and any(line.startswith(f"Beat {i}") or line.startswith(f"{i}.") or line.startswith(f"{i}:") for i in range(1, 19)):
                beat_num += 1
                directive = line.split(":", 1)[-1].strip() if ":" in line else line
                beats.append(StoryBeat(number=beat_num, directive=directive))
        
        return beats if beats else [StoryBeat(number=1, directive="Write the story")]


# Integrace do PrismQ workflow
def script_from_idea_title_moving_window(idea: dict, title: str) -> str:
    """
    Entry point pro PrismQ.T.Script.From.Title.Idea s Moving Window.
    
    PouÅ¾itÃ­:
        from T.Script.From.Idea.Title.MovingWindow import script_from_idea_title_moving_window
        
        script = script_from_idea_title_moving_window(idea, title)
    """
    generator = PrismQMovingWindowScript(
        model="qwen2.5:32b",
        window_size=1200,
        block_words=400
    )
    return generator.from_idea_and_title(idea, title)
```

#### Konfigurace pro rÅ¯znÃ© dÃ©lky scriptu

| DÃ©lka scriptu | PoÄet beats | Block size | Celkem slov |
|---------------|-------------|------------|-------------|
| **KrÃ¡tkÃ½** (Reddit story) | 5-8 | 300 slov | 1,500-2,400 |
| **StÅ™ednÃ­** (Short story) | 10-12 | 400 slov | 4,000-4,800 |
| **DlouhÃ½** (Novella) | 15-20 | 500 slov | 7,500-10,000 |
| **Velmi dlouhÃ½** (Novel chapter) | 20-30 | 600 slov | 12,000-18,000 |

#### DoporuÄenÃ½ workflow s Moving Window

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PrismQ.T Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Idea.Creation                                           â”‚
â”‚       â†“                                                     â”‚
â”‚  2. Story.From.Idea (creates Story Bible)                   â”‚
â”‚       â†“                                                     â”‚
â”‚  3. Title.From.Idea                                         â”‚
â”‚       â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 4. Script.From.Title.Idea                            â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚    â”‚ ğŸ“– MOVING WINDOW GENERATOR                  â”‚   â”‚   â”‚
â”‚  â”‚    â”‚                                             â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Story Bible â† Idea + Title               â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Outline (12 beats)                       â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ For each beat:                           â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Generate Block                         â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Summarize                              â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Extract Facts                          â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Prepare Directive                      â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Combine â†’ Script v1                      â”‚   â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â†“                                                     â”‚
â”‚  5. Review.Title.By.Script.Idea                             â”‚
â”‚       â†“                                                     â”‚
â”‚  6-16. Quality Reviews (Grammar, Tone, Content...)          â”‚
â”‚       â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 17-18. Story.Review + Story.Polish                   â”‚   â”‚
â”‚  â”‚    (Moving Window pro Polish refinement)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â†“                                                     â”‚
â”‚  Publishing                                                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Kdy pouÅ¾Ã­t Moving Window vs Standard Generation

| Situace | Metoda | DÅ¯vod |
|---------|--------|-------|
| Reddit Stories (<2000 slov) | Standard | RychlejÅ¡Ã­, kontext staÄÃ­ |
| Short Stories (2000-5000 slov) | Moving Window | Prevence memory drift |
| Novellas (5000-15000 slov) | Moving Window | **PovinnÃ©** |
| Novel chapters (15000+ slov) | Moving Window | **PovinnÃ©** |
| Title generation | Standard | KrÃ¡tkÃ½ vÃ½stup |
| Reviews | Standard | AnalytickÃ©, ne generativnÃ­ |
| Script refinement | Moving Window | ZachovÃ¡nÃ­ konzistence |

For RTX 5090 with 32GB VRAM, we recommend:

```bash
# Best quality for story writing
ollama pull qwen2.5:32b

# Alternative for SEO and metadata
ollama pull llama3.1:70b-q4_K_M
```

**PrismQ Configuration for RTX 5090:**

```python
from T.Publishing.SEO.Keywords import AIConfig

# High-quality story generation config
story_config = AIConfig(
    model="qwen2.5:32b",
    api_base="http://localhost:11434",
    temperature=0.7,  # Higher for creative writing
    max_tokens=2000,
    enable_ai=True
)

# SEO metadata config
seo_config = AIConfig(
    model="llama3.1:70b-q4_K_M",
    api_base="http://localhost:11434",
    temperature=0.3,  # Lower for consistent SEO output
    enable_ai=True
)
```

### Optimalizace naÄÃ­tÃ¡nÃ­ modelu (Model Loading Optimization)

Pro zamezenÃ­ opakovanÃ©ho naÄÃ­tÃ¡nÃ­ modelu do VRAM bÄ›hem bÄ›hu PrismQ:

#### Ollama Keep-Alive nastavenÃ­

Ollama standardnÄ› udrÅ¾uje model v pamÄ›ti 5 minut po poslednÃ­m dotazu. Pro delÅ¡Ã­ workflow:

```bash
# Nastavte OLLAMA_KEEP_ALIVE na delÅ¡Ã­ dobu (napÅ™. 60 minut)
export OLLAMA_KEEP_ALIVE=60m

# Nebo permanentnÄ› v .bashrc / .zshrc
echo 'export OLLAMA_KEEP_ALIVE=60m' >> ~/.bashrc
```

**Windows (PowerShell):**
```powershell
# Nastavte promÄ›nnou prostÅ™edÃ­
$env:OLLAMA_KEEP_ALIVE = "60m"

# Nebo permanentnÄ›
[System.Environment]::SetEnvironmentVariable("OLLAMA_KEEP_ALIVE", "60m", "User")
```

#### PrismQ Model Manager (doporuÄenÃ½ pÅ™Ã­stup)

Pro optimÃ¡lnÃ­ vÃ½kon pouÅ¾ijte jednotnÃ½ model pro celÃ½ workflow:

```python
import ollama

class PrismQModelManager:
    """SprÃ¡vce modelu pro efektivnÃ­ vyuÅ¾itÃ­ VRAM."""
    
    _instance = None
    _current_model = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def ensure_model_loaded(self, model_name: str):
        """NaÄte model pouze pokud jeÅ¡tÄ› nenÃ­ v pamÄ›ti."""
        if self._current_model != model_name:
            # Warmup dotaz pro naÄtenÃ­ modelu do VRAM
            ollama.chat(
                model=model_name,
                messages=[{"role": "user", "content": "Hello"}],
                options={"num_predict": 1}
            )
            self._current_model = model_name
            print(f"Model {model_name} naÄten do VRAM")
        return self._current_model

# PouÅ¾itÃ­ na zaÄÃ¡tku workflow
manager = PrismQModelManager.get_instance()
manager.ensure_model_loaded("qwen2.5:32b")

# VÅ¡echny nÃ¡sledujÃ­cÃ­ dotazy pouÅ¾ijÃ­ jiÅ¾ naÄtenÃ½ model
```

#### DoporuÄenÃ¡ strategie pro celÃ½ PrismQ workflow

| FÃ¡ze | Model | DÅ¯vod |
|------|-------|-------|
| **Idea â†’ Title â†’ Script â†’ Review** | `qwen2.5:32b` | Jeden model pro celÃ½ workflow, bez pÅ™epÃ­nÃ¡nÃ­ |
| **SEO Metadata** (volitelnÄ›) | PÅ™epnout na `llama3.3:70b-q4_K_M` | Pouze pokud je nutnÃ¡ lepÅ¡Ã­ SEO kvalita |

> **Tip:** Pro maximÃ¡lnÃ­ efektivitu pouÅ¾Ã­vejte jeden model pro celÃ½ bÄ›h. PÅ™epÃ­nÃ¡nÃ­ mezi modely vyÅ¾aduje uvolnÄ›nÃ­ a naÄtenÃ­ ~20-40GB dat, coÅ¾ trvÃ¡ 10-30 sekund.

### OptimÃ¡lnÃ­ konfigurace pro Ryzen 9 9900X3D + RTX 5090

Pro vÃ¡Å¡ konkrÃ©tnÃ­ hardware (AMD Ryzen 9 9900X3D + RTX 5090 32GB):

| Parametr | DoporuÄenÃ¡ hodnota | DÅ¯vod |
|----------|-------------------|-------|
| **Model** | `qwen2.5:32b` nebo `llama3.3:70b-q4_K_M` | PlnÄ› vyuÅ¾ije 32GB VRAM |
| **Kvantizace (70B)** | Q4_K_M | OptimÃ¡lnÃ­ pro 32GB VRAM |
| **Context Length** | 8192-16384 | VyuÅ¾ije 3D V-Cache pro KV cache |
| **Batch Size** | 1 | StandardnÃ­ pro generovÃ¡nÃ­ |
| **GPU Layers** | All (auto) | CelÃ½ model na GPU |

```bash
# OptimÃ¡lnÃ­ Ollama konfigurace pro Ryzen 9 9900X3D + RTX 5090
export OLLAMA_NUM_PARALLEL=1          # Jeden request najednou
export OLLAMA_KEEP_ALIVE=60m          # Model zÅ¯stane v pamÄ›ti
export OLLAMA_MAX_LOADED_MODELS=1     # Jeden model najednou (Å¡etÅ™Ã­ VRAM)

# SpusÅ¥te Ollama
ollama serve
```

**VyuÅ¾itÃ­ 3D V-Cache (141MB):**
Ryzen 9 9900X3D mÃ¡ masivnÃ­ L3 cache, kterÃ¡ pomÃ¡hÃ¡ s:
- RychlejÅ¡Ã­m tokenizaÄnÃ­m pre/post-processingem
- EfektivnÄ›jÅ¡Ã­m CPU offloadingem (pokud potÅ™ebnÃ½)
- NiÅ¾Å¡Ã­ latencÃ­ pÅ™i komunikaci s GPU

### Using MPT-7B-StoryWriter with HuggingFace

If you prefer the specialized MPT-7B-StoryWriter model:

```bash
pip install transformers torch accelerate
```

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "mosaicml/mpt-7b-storywriter"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

prompt = "Write the opening chapter of a dark horror story set in a small Czech town:"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=1000,
    temperature=0.7,
    do_sample=True
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

> **Note**: MPT-7B-StoryWriter requires manual setup with HuggingFace Transformers and is not directly supported by Ollama. For easier integration with PrismQ, we recommend Qwen2.5:32b.

## Step 3 â€“ Test the Model

After the download finishes, verify the installation:

```bash
ollama run qwen2.5:14b
```

You'll enter an interactive prompt. Try a test query:

```
Write a dark, emotional horror story opening set in a small Czech town at night.
```

If it responds with a story, Qwen2.5-14B is correctly installed. Type `/bye` to exit.

## Step 4 â€“ Python Integration

### Install the Ollama Python Package

```bash
pip install ollama
```

### Minimal Python Script

Create a file named `story_test.py`:

```python
import ollama

prompt = "Write the first 500 words of a psychological horror story told in first person."

response = ollama.chat(
    model="qwen2.5:14b",
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response["message"]["content"])
```

Run it:

```bash
python story_test.py
```

This directly uses your local Qwen2.5-14B via Ollama's HTTP API on `localhost:11434`.

## PrismQ Integration

### Using AI in SEO Metadata Generation

PrismQ's SEO Keywords module already supports Ollama integration. Example usage:

```python
from T.Publishing.SEO.Keywords import process_content_seo, AIConfig

# Configure to use Qwen2.5-14B
config = AIConfig(
    model="qwen2.5:14b",
    api_base="http://localhost:11434",
    temperature=0.3,
    enable_ai=True
)

result = process_content_seo(
    title="Your Content Title",
    script="Your content script...",
    use_ai=True,
    ai_config=config,
    brand_name="Your Brand"
)

print(result['meta_description'])
print(result['title_tag'])
```

### Default AI Configuration

The default configuration in PrismQ varies by module:
- **SEO Keywords Module**: Uses `llama3.1:70b-q4_K_M` (optimized for SEO tasks)
- **General Content Generation**: Recommended `qwen2.5:14b` (best for creative writing)

Common settings:
- **API Base**: `http://localhost:11434`
- **Temperature**: 0.3 (lower for more consistent output)

You can customize these settings using the `AIConfig` class to match your model choice.

## Troubleshooting

### Ollama Not Running

If you see errors about Ollama being unavailable:

1. **Windows**: Check if Ollama is running in the system tray
2. **macOS**: Launch Ollama from Applications
3. **Linux**: Start the service with `ollama serve`

### Model Not Found

If a model is not found, pull it first:

```bash
ollama pull qwen2.5:14b
```

### Out of Memory Errors

If you encounter memory errors:
- Use a smaller model variant (e.g., `qwen2.5:7b` instead of `qwen2.5:14b`)
- Close other GPU-intensive applications
- Increase system swap space

### Slow Response Times

For faster inference:
- Ensure you're using GPU acceleration (NVIDIA CUDA or Apple Metal)
- Consider using quantized model variants
- Reduce `max_tokens` in the configuration

## API Reference

Ollama exposes a REST API at `http://localhost:11434`. Key endpoints:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/generate` | POST | Generate text completion |
| `/api/chat` | POST | Chat-style completion |
| `/api/tags` | GET | List available models |
| `/api/pull` | POST | Download a model |

### Example API Call

```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "qwen2.5:14b",
        "prompt": "Write a short story about...",
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 500
        }
    },
    timeout=30
)

result = response.json()
print(result["response"])
```

## Related Documentation

- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System architecture overview
- **[T Module README](../../T/README.md)** - Text generation pipeline
- **[SEO Keywords Module](../../T/Publishing/SEO/Keywords/README.MD)** - SEO Keywords module documentation

## Version History

### 1.0.0 (2025-12-05)
- Initial documentation
- Ollama installation instructions
- Qwen2.5-14B model setup
- Python integration examples
