# AI Models Setup Guide

**Document Type**: Setup Guide  
**Scope**: Project-wide  
**Last Updated**: 2025-12-05

## Overview

PrismQ uses local LLM models through Ollama for AI-powered content generation and SEO metadata optimization. This guide covers how to set up Ollama and configure AI models for use with PrismQ.

### ğŸ† Primary Local AI Model: Qwen 3:30B

**PrismQ uses Qwen 3:30B as the primary local AI model** for all generation and review tasks. This model provides the best balance of quality, speed, and VRAM efficiency for RTX 5090 systems.

| Aspect | Qwen 3:30B |
|--------|------------|
| **Role** | Primary local AI for generation & review |
| **Parameters** | 30B |
| **VRAM Usage** | ~19GB (fits comfortably in RTX 5090 32GB) |
| **Quality** | â­â­â­â­â­ |
| **Speed** | Fast |
| **Best For** | Script generation, Story review, Title creation, Idea generation |

```bash
# Quick start - Pull the primary model
ollama pull qwen3:32b
```

## Prerequisites

- Windows, macOS, or Linux operating system
- At least 16GB RAM (32GB recommended for larger models)
- GPU with sufficient VRAM for model inference (RTX 5090 with 32GB VRAM recommended)
- Stable internet connection for model downloads

## Step 1 â€“ Install Ollama

Download and install Ollama from the official website:

**Download Link**: https://ollama.com/download

### Windows Installation
1. Download the Windows installer from the link above
2. Run the installer and follow the prompts
3. Ollama will be available as a system service

### macOS Installation
1. Download the macOS app from the link above
2. Drag Ollama to your Applications folder
3. Launch Ollama from Applications

### Linux Installation
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## Step 2 â€“ Pull the Qwen 3:30B Model (Primary)

**Qwen 3:30B is the primary local AI model for PrismQ.** It handles all generation and review tasks with excellent quality.

In your terminal, run:

```bash
ollama pull qwen3:32b
```

This will download approximately 19GB of model weights. The download time depends on your internet connection speed.

### Test the Model

After the download finishes, verify the installation:

```bash
ollama run qwen3:32b
```

You'll drop into an interactive prompt. Try something like:

```
Write a dark, emotional horror story opening set in a small Czech town at night.
```

If it responds with quality output, Qwen 3:30B is correctly installed.

### Qwen 3 Model Variants

| Model | Size | VRAM Required | Use Case |
|-------|------|---------------|----------|
| `qwen3:8b` | ~5GB | 8GB | Lighter weight, faster inference |
| `qwen3:14b` | ~9GB | 16GB | Good balance for mid-range GPUs |
| **`qwen3:32b`** | **~19GB** | **24GB+** | **ğŸ† PRIMARY - Best quality for RTX 5090** |
| `qwen3:72b` | ~45GB | 48GB+ | Premium quality, requires multi-GPU or quantization |

### Alternative: Qwen 2.5 Models (Legacy)

For systems that cannot run Qwen 3, Qwen 2.5 variants are available:

| Model | Size | VRAM Required | Use Case |
|-------|------|---------------|----------|
| `qwen2.5:7b` | ~4.5GB | 8GB | Lighter weight, faster inference |
| `qwen2.5:14b` | ~9GB | 16GB | Good balance of quality and speed |
| `qwen3:32b` | ~20GB | 24GB+ | High quality alternative |

```bash
# Legacy fallback
ollama pull qwen2.5:14b
```

### Other Supported Models

PrismQ also supports other Ollama models. For SEO-specific tasks, the Keywords module uses:

| Model | Size | VRAM Required | Use Case |
|-------|------|---------------|----------|
| `llama3.1:70b-q4_K_M` | ~40GB | 48GB+ | SEO metadata generation (Keywords module default) |

To pull this model:
```bash
ollama pull llama3.1:70b-q4_K_M
```

## Model Comparison for High-End Systems (RTX 5090)

For users with an NVIDIA RTX 5090 (32GB VRAM), you have access to the most powerful local AI models. Here's a comprehensive comparison for story writing and content generation:

### ğŸ† Primary Model: Qwen 3:30B

**Qwen 3:30B is the recommended primary model for all PrismQ tasks on RTX 5090.**

| Aspect | Qwen 3:30B |
|--------|------------|
| **Parameters** | 30B |
| **VRAM Usage** | ~19GB |
| **Story Quality** | â­â­â­â­â­ |
| **Speed** | Fast |
| **Context** | 32K tokens |
| **Strengths** | Creative writing, review, generation |

```bash
# Install primary model
ollama pull qwen3:32b
```

### Recommended Models for RTX 5090

| Model | Parameters | VRAM Usage | Story Quality | Speed | Best For |
|-------|------------|------------|---------------|-------|----------|
| **ğŸ† Qwen3:30b** | 30B | ~19GB | â­â­â­â­â­ | Fast | **PRIMARY - All PrismQ tasks** |
| **Qwen3:72b-q4** | 72B | ~45GB | â­â­â­â­â­ | Medium | Premium quality with quantization |
| **Qwen2.5:32b** | 32B | ~20GB | â­â­â­â­â­ | Medium | Legacy fallback |
| **Llama3.3:70b-q4** | 70B | ~40GB | â­â­â­â­â­ | Slow | Alternative for specific tasks |

### Qwen 3 vs Qwen 2.5 Comparison

| Aspect | Qwen 2.5 | Qwen 3 | Improvement |
|--------|----------|--------|-------------|
| **Long-term coherence** | Good | Excellent | +40% |
| **English quality** | Good | Excellent | +35% |
| **World-building rules** | Moderate | Strong | Less hallucinations |
| **Dramatic structure** | Good | Excellent | +40% |
| **Lyrical imagery** | Moderate | Strong | +40% |
| **Emotional depth** | Good | Excellent | +30% |
| **Natural dialogue** | Good | Excellent | +35% |

> **Conclusion:** Qwen 3:30B provides ~30-40% improvement over Qwen 2.5:32b for creative writing tasks.

### Llama 3.1 405B vs Llama 3.3 70B

| Aspect | Llama 3.1 â€” 405B | Llama 3.3 â€” 70B |
|--------|------------------|-----------------|
| **Parameters** | 405B | 70B |
| **VRAM Required** | ~200GB+ (requires multi-GPU or cloud) | ~40GB (Q4 quantized) |
| **Context Length** | 128K tokens | 128K tokens |
| **Quality** | â­â­â­â­â­ (best-in-class) | â­â­â­â­â­ (excellent) |
| **Speed** | Very Slow | Medium |
| **Local RTX 5090** | âŒ Too large | âœ… With quantization |
| **Ollama Support** | âŒ Cloud/API only | âœ… `ollama pull llama3.3:70b` |

### Understanding Quantization

**Co je kvantizace?** Kvantizace je technika komprese modelu, kterÃ¡ sniÅ¾uje pÅ™esnost vah (napÅ™. z 16-bit na 4-bit). To vÃ½raznÄ› zmenÅ¡Ã­ velikost modelu a VRAM poÅ¾adavky.

#### KompletnÃ­ pÅ™ehled kvantizaÄnÃ­ch variant

| Kvantizace | Kvalita vs OriginÃ¡l | VRAM (70B model) | Rychlost | DoporuÄenÃ­ |
|------------|---------------------|------------------|----------|------------|
| **FP16** (bez kvantizace) | 100% | ~140GB | NejpomalejÅ¡Ã­ | âŒ PÅ™Ã­liÅ¡ velkÃ© pro RTX 5090 |
| **Q8_0** (8-bit) | ~99.5% | ~70GB | PomalÃ¡ | âŒ PÅ™Ã­liÅ¡ velkÃ© pro RTX 5090 |
| **Q6_K** (6-bit) | ~98.5% | ~54GB | StÅ™ednÃ­ | âš ï¸ Na hranici, mÅ¯Å¾e fungovat s offloadingem |
| **Q5_K_M** (5-bit) | ~97% | ~47GB | RychlÃ¡ | âœ… DobrÃ¡ volba pro kvalitu |
| **Q5_K_S** (5-bit small) | ~96% | ~45GB | RychlÃ¡ | âœ… DobrÃ¡ alternativa |
| **Q4_K_M** (4-bit medium) | ~95% | ~40GB | Velmi rychlÃ¡ | âœ… **DOPORUÄŒENO pro RTX 5090** |
| **Q4_K_S** (4-bit small) | ~94% | ~38GB | Velmi rychlÃ¡ | âœ… NejrychlejÅ¡Ã­ kvalitnÃ­ varianta |
| **Q3_K_M** (3-bit) | ~90% | ~33GB | ExtrÃ©mnÄ› rychlÃ¡ | âš ï¸ ZnatelnÃ¡ ztrÃ¡ta kvality |
| **IQ4_XS** (4-bit i-quant) | ~94.5% | ~36GB | Velmi rychlÃ¡ | âœ… ModernÃ­ alternativa k Q4 |

#### ğŸ† FinÃ¡lnÃ­ doporuÄenÃ­ pro RTX 5090 (32GB VRAM)

**Pro maximÃ¡lnÃ­ kvalitu:** `Q4_K_M` nebo `Q5_K_S`
- Q4_K_M nabÃ­zÃ­ nejlepÅ¡Ã­ pomÄ›r kvalita/VRAM pro 32GB karty
- RozdÃ­l mezi Q4_K_M a Q6_K je v praxi tÃ©mÄ›Å™ nepostÅ™ehnutelnÃ½ pro kreativnÃ­ psanÃ­
- Q6_K je pÅ™Ã­liÅ¡ velkÃ½ pro RTX 5090 bez CPU offloadingu

```bash
# ğŸ† NEJLEPÅ Ã VOLBA pro RTX 5090 32GB - Llama 3.3 70B Q4_K_M
ollama pull llama3.3:70b-q4_K_M

# Alternativa pro o nÄ›co vyÅ¡Å¡Ã­ kvalitu (pomalejÅ¡Ã­)
ollama pull llama3.3:70b-q5_K_S

# Pro Qwen2.5 32B (vejde se celÃ½ bez kvantizace)
ollama pull qwen3:32b
```

> **PoznÃ¡mka ke Q6_K:** I kdyÅ¾ Q6_K nabÃ­zÃ­ ~98.5% kvality, vyÅ¾aduje ~54GB VRAM pro 70B model. Na RTX 5090 (32GB) by musel pouÅ¾Ã­t CPU offloading, coÅ¾ dramaticky zpomalÃ­ inference. Pro vaÅ¡i sestavu doporuÄuji Q4_K_M - ztrÃ¡ta kvality je minimÃ¡lnÃ­ (~5%) a rychlost bude vÃ½raznÄ› lepÅ¡Ã­.

### Model Recommendations by PrismQ Task

**Primary Model: Qwen 3:30B** is used for all PrismQ tasks for consistency and optimal performance.

| Task | Primary Model | Alternative | Why |
|------|---------------|-------------|-----|
| **Idea Generation** | **Qwen3:30b** | Llama 3.3:70b | Strong creative reasoning, diverse ideas |
| **Title Creation** | **Qwen3:30b** | Qwen2.5:14b | Fast, concise outputs, good for iteration |
| **Script Writing** | **Qwen3:30b** | Qwen2.5:32b | Best narrative quality, instruction following |
| **Review/Editing** | **Qwen3:30b** | Llama 3.3:70b | Superior analytical and reasoning capabilities |
| **SEO Metadata** | **Qwen3:30b** | Llama 3.1:70b-q4 | Consistent, structured outputs |

> **Note:** Using a single model (Qwen 3:30B) for all tasks avoids VRAM reloading delays and maintains consistency across the workflow.

#### Task-Specific Configuration:

```python
from T.Publishing.SEO.Keywords import AIConfig

# Primary model for all tasks
PRIMARY_MODEL = "qwen3:32b"

# Idea Generation - creative, diverse
idea_config = AIConfig(
    model=PRIMARY_MODEL,
    temperature=0.8,  # Higher for creativity
    enable_ai=True
)

# Title Creation - fast iteration
title_config = AIConfig(
    model=PRIMARY_MODEL,
    temperature=0.5,
    enable_ai=True
)

# Script Writing - high quality narrative
script_config = AIConfig(
    model=PRIMARY_MODEL,
    temperature=0.7,
    max_tokens=4000,
    enable_ai=True
)

# Review/Editing - analytical
review_config = AIConfig(
    model=PRIMARY_MODEL,
    temperature=0.3,  # Lower for consistent analysis
    enable_ai=True
)
```

#### When to Use Llama 3.1 405B:
- Cloud/API access (Together AI, Fireworks, Groq)
- Maximum quality requirements
- Complex reasoning tasks
- Enterprise production workflows

#### When to Use Llama 3.3 70B:
- **Recommended for local RTX 5090** âœ…
- Best balance of quality and local inference
- Reviews and analytical tasks
- Idea generation with strong reasoning

```bash
# Install Llama 3.3 70B for local use
ollama pull llama3.3:70b-q4_K_M
```

### MPT-7B-StoryWriter (HuggingFace)

The [MPT-7B-StoryWriter](https://huggingface.co/mosaicml/mpt-7b-storywriter) is a specialized model fine-tuned specifically for story writing with an impressive 65K context length.

| Aspect | MPT-7B-StoryWriter | Qwen2.5:32b |
|--------|-------------------|-------------|
| **Parameters** | 7B | 32B |
| **VRAM Required** | ~8GB | ~20GB |
| **Context Length** | 65,536 tokens | 32,768 tokens |
| **Story Quality** | â­â­â­â­ (specialized) | â­â­â­â­â­ (general) |
| **Inference Speed** | Very Fast | Medium |
| **Ollama Support** | âŒ (requires manual setup) | âœ… Native |
| **Best Use Case** | Long-form novels, extended narratives | All creative content |

#### When to Choose MPT-7B-StoryWriter:
- Writing very long stories (novels, extended series)
- Need 65K context for maintaining consistency
- Prefer faster inference over raw quality
- Working with HuggingFace Transformers directly

#### When to Choose Qwen2.5:32b:
- General creative writing (stories, scripts, dialogue)
- Need better instruction following
- Prefer easy Ollama integration
- Want highest quality output

### ğŸ“š Modely optimalizovanÃ© pro tvorbu pÅ™Ã­bÄ›hÅ¯

Pro kreativnÃ­ psanÃ­ a tvorbu pÅ™Ã­bÄ›hÅ¯ existujÃ­ specializovanÃ© modely s lepÅ¡Ã­m vÃ½konem neÅ¾ obecnÃ© LLM:

#### ğŸ§ª ReÃ¡lnÃ© testy modelÅ¯ (English YA Fiction)

Na zÃ¡kladÄ› internÃ­ho testovÃ¡nÃ­ PrismQ s anglickÃ½mi pÅ™Ã­bÄ›hy pro US/CA publikum:

| Model | Test pÅ™Ã­bÄ›h | SkÃ³re | Struktura | Dialog | Postavy | NapÄ›tÃ­ | Konzistence | ÄŒitelnost | PoznÃ¡mka |
|-------|-------------|-------|-----------|--------|---------|--------|-------------|-----------|----------|
| **qwen3:32b (EN)** | The Lighthouse Keeper's Secret | **7.8/10** | 8 | 9 | 6.5 | 8 | 7 | 7.5 | 7 | ğŸ† PÅ™ekvapivÄ› ÄistÃ©, soudrÅ¾nÃ©, ÄtivÃ© â€” mnohem lepÅ¡Ã­ neÅ¾ CZ |
| **mistral-nemo:12b (EN)** | The Whispering Grove | **7.5/10** | 8 | 9 | 6 | 7.5 | 6 | 7 | 6 | AngliÄtina vÃ½raznÄ› zvedÃ¡ kvalitu, dobrÃ© YA-fantasy |

> **KlÃ­ÄovÃ© zjiÅ¡tÄ›nÃ­:** AnglickÃ¡ verze vÃ½raznÄ› pÅ™evyÅ¡uje Äeskou u obou modelÅ¯. Pro US/CA publikum doporuÄujeme vÅ¾dy generovat v angliÄtinÄ›.

#### ğŸ† FinÃ¡lnÃ­ doporuÄenÃ­ pro English YA Fiction

Na zÃ¡kladÄ› testÅ¯ doporuÄujeme pro **americkÃ©/kanadskÃ© teen publikum**:

| Priorita | Model | SkÃ³re | NejlepÅ¡Ã­ pro |
|----------|-------|-------|--------------|
| **#1** | **Qwen2.5:32b** | 7.8/10 | Family Drama, Teen Drama, Mystery |
| **#2** | **Mistral-Nemo:12b** | 7.5/10 | YA Fantasy, Slice of Life, Reddit Stories |

```bash
# PrimÃ¡rnÃ­ model pro EN YA content
ollama pull qwen3:32b

# SekundÃ¡rnÃ­/rychlejÅ¡Ã­ model
ollama pull mistral-nemo:12b
```

#### ğŸ” SrovnÃ¡nÃ­ 32B modelÅ¯ pro kreativnÃ­ psanÃ­

| Model | Fine-tuning | Kvalita prÃ³zy | Kontext | AngliÄtina | Ollama | Benchmarks |
|-------|-------------|---------------|---------|------------|--------|------------|
| **Qwen2.5:32b-Instruct** | General | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 83.5 |
| **Yi-1.5-34B-Chat** | Chat/Creative | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 81.2 |
| **DeepSeek-V2-Lite (27B)** | General | â­â­â­â­ | 128K | DobrÃ¡ | âœ… | MMLU: 79.8 |
| **Mixtral-8x7B (47B MoE)** | Instruct | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 81.1 |
| **Command-R (35B)** | RAG/Chat | â­â­â­â­ | 128K | VÃ½bornÃ¡ | âœ… | MMLU: 78.5 |

#### ğŸ“– Fine-tuned modely pro kreativnÃ­ psanÃ­ (32B tÅ™Ã­da)

| Model | Specializace | Kvalita | VRAM | Zdroj |
|-------|--------------|---------|------|-------|
| **Nous-Hermes-2-Yi-34B** | Creative writing, RP | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Airoboros-34B** | Creative, storytelling | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Dolphin-2.6-Yi-34B** | Uncensored creative | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Goliath-120B** (merged) | Premium creative | â­â­â­â­â­ | ~70GB | HuggingFace |
| **Chronos-Hermes-34B** | Long-form fiction | â­â­â­â­â­ | ~22GB | HuggingFace |
| **StellarBright-Qwen2.5-32B** | Creative writing | â­â­â­â­â­ | ~20GB | HuggingFace |

> **PoznÃ¡mka:** Fine-tuned modely pro kreativnÃ­ psanÃ­ Äasto pÅ™ekonÃ¡vajÃ­ vÄ›tÅ¡Ã­ obecnÃ© modely v kvalitÄ› narativu.

#### ğŸ¯ DoporuÄenÃ­ pro cÃ­lovÃ© publikum: Teen/Young Adult (10-20, US Å¾eny)

Pro americkÃ© a kanadskÃ© anglicky mluvÃ­cÃ­ publikum (pÅ™edevÅ¡Ã­m mladÃ© Å¾eny 10-20 let):

| Å½Ã¡nr | ğŸ† DoporuÄenÃ½ model | Alternativa | ProÄ |
|------|---------------------|-------------|------|
| **Reddit Stories** | Nous-Hermes-2-Yi-34B | Qwen2.5:32b | AutentickÃ½ Reddit styl, relatable |
| **Family Drama** | Qwen2.5:32b | Airoboros-34B | EmocionÃ¡lnÃ­ hloubka, realistickÃ© dialogy |
| **Teen Drama** | Dolphin-2.6-Yi-34B | Nous-Hermes-2-Yi-34B | Teen hlas, modernÃ­ slang |
| **Teen Stories** | Nous-Hermes-2-Yi-34B | Mistral-Nemo | YA narativ, engagement |
| **Romance (YA)** | Chronos-Hermes-34B | Qwen2.5:32b | EmotivnÃ­, clean romance |
| **Thriller/Mystery** | Qwen2.5:32b | Command-R | NapÄ›tÃ­, twist endings |
| **AITA/Confession** | Nous-Hermes-2-Yi-34B | Dolphin-2.6-Yi-34B | AutentickÃ½ POV |
| **Slice of Life** | Mistral-Nemo | Qwen2.5:32b | KaÅ¾dodennÃ­ situace, relatability |

#### ğŸ† TOP 3 pro Teen/YA obsah na RTX 5090

**1. Nous-Hermes-2-Yi-34B** - NejlepÅ¡Ã­ pro Reddit/Teen stories
```bash
ollama pull nous-hermes2:yi-34b-q4_K_M
```
- SpecializovanÃ½ na creative writing a roleplay
- PÅ™irozenÃ½ teen dialog a POV
- VÃ½bornÃ½ pro AITA, relationship drama, confession stories
- ~22GB VRAM (Q4)

**2. Qwen2.5:32b** - UniverzÃ¡lnÃ­ vysokÃ¡ kvalita
```bash
ollama pull qwen3:32b
```
- NejlepÅ¡Ã­ balance kvality a rychlosti
- VÃ½bornÃ¡ angliÄtina, emotivnÃ­ prÃ³za
- IdeÃ¡lnÃ­ pro family drama, romance
- ~20GB VRAM

**3. Dolphin-2.6-Yi-34B** - Pro autentickÃ½ teen hlas
```bash
ollama pull dolphin2.6:yi-34b-q4_K_M
```
- Uncensored, pÅ™irozenÃ© dialogy
- ModernÃ­ slang a teen expressions
- VhodnÃ½ pro edgier teen drama
- ~22GB VRAM (Q4)

#### ğŸ“Š Statistiky a benchmarky 32B modelÅ¯ (creative writing)

Na zÃ¡kladÄ› komunitnÃ­ch testÅ¯ a r/LocalLLaMA:

| Model | Reddit Stories | Dialogy | Emotivnost | Konzistence | CelkovÄ› |
|-------|----------------|---------|------------|-------------|---------|
| Nous-Hermes-2-Yi-34B | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | **#1** |
| Qwen2.5:32b | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | **#2** |
| Airoboros-34B | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | **#3** |
| Yi-34B-Chat | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | **#4** |

#### Konfigurace pro Teen/YA content

```python
from T.Publishing.SEO.Keywords import AIConfig

# Reddit Stories / AITA style
reddit_config = AIConfig(
    model="nous-hermes2:yi-34b-q4_K_M",
    temperature=0.85,           # VyÅ¡Å¡Ã­ pro autenticitu
    max_tokens=3000,
    enable_ai=True
)

# Teen Drama / Family Drama
drama_config = AIConfig(
    model="qwen3:32b",
    temperature=0.75,
    max_tokens=4000,
    enable_ai=True
)

# Teen Stories (YA fiction)
teen_stories_config = AIConfig(
    model="nous-hermes2:yi-34b-q4_K_M",
    temperature=0.8,
    max_tokens=5000,           # DelÅ¡Ã­ kapitoly
    enable_ai=True
)
```

#### Instalace modelÅ¯ pro Teen/YA content

```bash
# KompletnÃ­ sada pro Teen/YA publikum na RTX 5090

# 1. PrimÃ¡rnÃ­ pro Reddit stories a teen drama
ollama pull nous-hermes2:yi-34b-q4_K_M

# 2. UniverzÃ¡lnÃ­ vysokÃ¡ kvalita
ollama pull qwen3:32b

# 3. Pro edgier teen content
ollama pull dolphin2.6:yi-34b-q4_K_M

# 4. Pro dlouhÃ© sÃ©rie
ollama pull yi:34b-chat-q4_K_M
```

> **Tip pro US teen publikum:** PouÅ¾Ã­vejte `temperature=0.8-0.9` pro autentiÄtÄ›jÅ¡Ã­ dialogy. Teen content vyÅ¾aduje souÄasnÃ½ slang, pop culture reference a relatable situace.

#### SrovnÃ¡nÃ­ modelÅ¯ pro psanÃ­ pÅ™Ã­bÄ›hÅ¯

| Model | Parametry | VRAM | Kvalita pÅ™Ã­bÄ›hÅ¯ | Kontext | Ollama | DoporuÄenÃ­ |
|-------|-----------|------|-----------------|---------|--------|------------|
| **Mistral-Nemo-Instruct-2407** | 12B | ~8GB | â­â­â­â­â­ | 128K | âœ… | ğŸ† **NEJLEPÅ Ã pro pÅ™Ã­bÄ›hy** |
| **Qwen2.5:32b** | 32B | ~20GB | â­â­â­â­â­ | 32K | âœ… | VÃ½bornÃ¡ kvalita, versatilnÃ­ |
| **Llama-3.1-Storm-8B** | 8B | ~6GB | â­â­â­â­ | 8K | âœ… | KreativnÃ­, rychlÃ½ |
| **Nous-Hermes-2-Mixtral** | 47B | ~28GB | â­â­â­â­â­ | 32K | âœ… | NejlepÅ¡Ã­ MoE pro pÅ™Ã­bÄ›hy |
| **Yi-34B-Chat** | 34B | ~22GB | â­â­â­â­â­ | 200K | âœ… | ExtrÃ©mnÃ­ kontext |
| **DeepSeek-Coder-V2-Lite** | 16B | ~10GB | â­â­â­â­ | 128K | âœ… | DobrÃ½ pro dialogy |
| **MPT-7B-StoryWriter** | 7B | ~8GB | â­â­â­â­ | 65K | âŒ | SpecializovanÃ½ na romÃ¡ny |
| **Fimbulvetr-11B** | 11B | ~8GB | â­â­â­â­â­ | 8K | âš ï¸ | VÃ½jimeÄnÃ½ pro RP/fiction |

#### ğŸ† Top 3 doporuÄenÃ© modely pro pÅ™Ã­bÄ›hy na RTX 5090

**1. Mistral-Nemo-Instruct-2407** - NejlepÅ¡Ã­ volba
```bash
ollama pull mistral-nemo:12b
```
- 128K tokenÅ¯ kontextu (perfektnÃ­ pro dlouhÃ© pÅ™Ã­bÄ›hy)
- VÃ½jimeÄnÃ¡ kreativita a koherence
- OptimalizovÃ¡no pro narativnÃ­ Ãºlohy
- Vejde se do 32GB VRAM bez kvantizace

**2. Nous-Hermes-2-Mixtral-8x7B** - Premium kvalita
```bash
ollama pull nous-hermes2-mixtral:8x7b-q4_K_M
```
- MoE architektura (efektivnÃ­ vyuÅ¾itÃ­ parametrÅ¯)
- Å piÄkovÃ¡ kvalita prÃ³zy
- VyÅ¾aduje ~28GB VRAM

**3. Yi-34B-Chat** - Pro extrÃ©mnÄ› dlouhÃ© pÅ™Ã­bÄ›hy
```bash
ollama pull yi:34b-chat-q4_K_M
```
- 200K tokenÅ¯ kontextu (nejdelÅ¡Ã­)
- IdeÃ¡lnÃ­ pro romÃ¡ny a sÃ©rie
- VyÅ¾aduje ~22GB VRAM (Q4)

#### SpecializovanÃ© modely pro rÅ¯znÃ© Å¾Ã¡nry

| Å½Ã¡nr | DoporuÄenÃ½ model | Alternativa |
|------|------------------|-------------|
| **Horror/Dark Fantasy** | Mistral-Nemo | Fimbulvetr-11B |
| **Romantika** | Qwen2.5:32b | Yi-34B-Chat |
| **Sci-Fi** | Nous-Hermes-2-Mixtral | DeepSeek-V2 |
| **Detektivky** | Llama 3.3:70b | Mistral-Nemo |
| **DÄ›tskÃ© pÅ™Ã­bÄ›hy** | Qwen2.5:14b | Mistral-Nemo |
| **EpickÃ¡ fantasy** | Yi-34B-Chat | Nous-Hermes-2 |
| **KrÃ¡tkÃ© povÃ­dky** | Mistral-Nemo | Qwen2.5:32b |
| **Dialogy/ScÃ©nÃ¡Å™e** | Llama 3.3:70b | Qwen2.5:32b |

#### Instalace nejlepÅ¡Ã­ch modelÅ¯ pro pÅ™Ã­bÄ›hy

```bash
# KompletnÃ­ sada pro profesionÃ¡lnÃ­ tvorbu pÅ™Ã­bÄ›hÅ¯ na RTX 5090

# 1. PrimÃ¡rnÃ­ model pro pÅ™Ã­bÄ›hy (doporuÄeno)
ollama pull mistral-nemo:12b

# 2. Pro dlouhÃ© romÃ¡ny
ollama pull yi:34b-chat-q4_K_M

# 3. Pro premium kvalitu prÃ³zy
ollama pull nous-hermes2-mixtral:8x7b-q4_K_M

# 4. UniverzÃ¡lnÃ­ zÃ¡loha
ollama pull qwen3:32b
```

#### Konfigurace pro tvorbu pÅ™Ã­bÄ›hÅ¯

```python
from T.Publishing.SEO.Keywords import AIConfig

# OptimÃ¡lnÃ­ konfigurace pro kreativnÃ­ psanÃ­ pÅ™Ã­bÄ›hÅ¯
story_writing_config = AIConfig(
    model="mistral-nemo:12b",  # NejlepÅ¡Ã­ pro pÅ™Ã­bÄ›hy
    api_base="http://localhost:11434",
    temperature=0.8,           # VyÅ¡Å¡Ã­ pro kreativitu
    max_tokens=4000,           # DlouhÃ© kapitoly
    enable_ai=True
)

# Pro velmi dlouhÃ© pÅ™Ã­bÄ›hy (romÃ¡ny)
novel_config = AIConfig(
    model="yi:34b-chat-q4_K_M",
    temperature=0.7,
    max_tokens=8000,           # CelÃ© kapitoly
    enable_ai=True
)

# Pro dialogy a scÃ©nÃ¡Å™e
dialogue_config = AIConfig(
    model="llama3.3:70b-q4_K_M",
    temperature=0.6,           # KonzistentnÄ›jÅ¡Ã­ dialogy
    max_tokens=2000,
    enable_ai=True
)
```

> **Tip pro tvorbu pÅ™Ã­bÄ›hÅ¯:** PouÅ¾Ã­vejte vyÅ¡Å¡Ã­ `temperature` (0.7-0.9) pro kreativnÄ›jÅ¡Ã­ vÃ½stup. Pro konzistentnÃ­ postavy a zÃ¡pletky udrÅ¾ujte kontext a pouÅ¾Ã­vejte modely s dlouhÃ½m kontextovÃ½m oknem (Yi-34B, Mistral-Nemo).

### ğŸ“– Moving Window Text Generation (ProfesionÃ¡lnÃ­ pipeline pro romÃ¡ny)

OsvÄ›dÄenÃ¡ metoda pro psanÃ­ dlouhÃ½ch textÅ¯ (romÃ¡ny, fanfikce, sÃ©rie) bez pÃ¡du kvality.

#### ğŸ¯ ZÃ¡kladnÃ­ princip

Model **nikdy negeneruje celÃ½ text najednou**. MÃ­sto toho generuje "bloky" (250â€“600 slov) a po kaÅ¾dÃ©m bloku:
1. ShrnutÃ­ (summary)
2. Extrakce klÃ­ÄovÃ½ch faktÅ¯
3. PlÃ¡n dalÅ¡Ã­ho dÄ›je
4. Re-injekce postav a tÃ³nu

#### â­ ProfesionÃ¡lnÃ­ Pipeline (6 krokÅ¯)

**ğŸ”¹ Krok 1 â€” Outline (kostra pÅ™Ã­bÄ›hu)**

```
Write a detailed story outline of 10â€“18 beats.
Include:
- setting
- main character arc
- emotional beats
- conflict escalation
- climax
- resolution
Keep it high-level.
```
> PouÅ¾ij nejsilnÄ›jÅ¡Ã­ model (GPT-5.1 / Claude Sonnet)

**ğŸ”¹ Krok 2 â€” Story Bible (pravidla pÅ™Ã­bÄ›hu)**

ZÅ¯stÃ¡vÃ¡ v kontextu celou dobu:
- JmÃ©na, motivace postav
- JazykovÃ© preference
- Tone guide
- ZakÃ¡zanÃ© halucinace
- Pravidla svÄ›ta

**ğŸ”¹ Krok 3 â€” Moving Window Writing**

| Model | Window Size |
|-------|-------------|
| **Qwen 32B** | 1000â€“1500 tokenÅ¯ |
| **Mistral 12B** | 500â€“800 tokenÅ¯ |

**Proces pro kaÅ¾dÃ½ blok:**

```
1ï¸âƒ£ GENERATE BLOCK (300â€“500 slov)
   â†’ Model napÃ­Å¡e dalÅ¡Ã­ segment pÅ™Ã­bÄ›hu

2ï¸âƒ£ SUMMARIZE BLOCK (150â€“250 slov)
   â†’ ShrnutÃ­ slouÅ¾Ã­ jako kondenzovanÃ¡ pamÄ›Å¥

3ï¸âƒ£ EXTRACT FACTS ("story memory")
   Facts so far:
   - Clara moved to Willow Creek to escape city life.
   - Found Evelyn's letters in attic.
   - House shows supernatural behavior.
   - Primary emotional tone: melancholy + suspense.

4ï¸âƒ£ DIRECTIVE (nÃ¡vod na dalÅ¡Ã­ blok)
   Next segment should:
   - escalate tension
   - introduce secondary character
   - foreshadow climax
   - avoid info dumps
   - keep consistent voice

5ï¸âƒ£ REINJEKCE
   Do promptu dÃ¡Å¡:
   - Story bible
   - ShrnutÃ­ + fakta poslednÃ­ch blokÅ¯
   - Directive
   - UkÃ¡zka stylu
   â†’ "Now continue the story."

6ï¸âƒ£ OPAKUJ
   Dokud nemÃ¡Å¡ desÃ­tky tisÃ­c slov.
```

#### ğŸ“Š VÃ½sledky Moving Window vs Standard Generation

| Metoda | Max dÃ©lka | Kvalita | Memory drift | OpakovÃ¡nÃ­ |
|--------|-----------|---------|--------------|-----------|
| **Standard** | 1500â€“3000 slov | KlesÃ¡ | âš ï¸ VysokÃ½ | âš ï¸ ÄŒastÃ© |
| **Moving Window** | 50 000+ slov | StabilnÃ­ | âœ… Å½Ã¡dnÃ½ | âœ… Å½Ã¡dnÃ© |

**ZlepÅ¡enÃ­ s Moving Window:**
- âœ… Å½Ã¡dnÃ½ pÃ¡d kvality
- âœ… Å½Ã¡dnÃ½ memory drift
- âœ… Å½Ã¡dnÃ© kruhovÃ© opakovÃ¡nÃ­
- âœ… LepÅ¡Ã­ pacing a drama
- âœ… KonzistentnÃ­ voice
- âœ… 2â€“3Ã— vyÅ¡Å¡Ã­ originalita a hloubka

#### ğŸš€ Qwen 3 vs Qwen 2.5

| Aspekt | Qwen 2.5 | Qwen 3 | ZlepÅ¡enÃ­ |
|--------|----------|--------|----------|
| DlouhodobÃ¡ koherence | â­â­â­ | â­â­â­â­â­ | +40% |
| AngliÄtina | â­â­â­â­ | â­â­â­â­â­ | +25% |
| SvÄ›tovÃ¡ pravidla (mÃ©nÄ› halucinacÃ­) | â­â­â­ | â­â­â­â­â­ | +35% |
| DramatickÃ¡ vÃ½stavba | â­â­â­â­ | â­â­â­â­â­ | +30% |
| Lyrika a imagery | â­â­â­ | â­â­â­â­â­ | +40% |
| PrÃ¡ce s emocemi | â­â­â­â­ | â­â­â­â­â­ | +30% |
| PÅ™irozenÃ¡ Å™eÄ | â­â­â­â­ | â­â­â­â­â­ | +25% |

> **Qwen 3 32B** â†’ srovnatelnÃ½ s GPT-3.5 Turbo v angliÄtinÄ›  
> **Qwen 3 72B/110B** â†’ blÃ­zko GPT-4.1 stylu tvorby pÅ™Ã­bÄ›hÅ¯

#### ğŸ§© DoporuÄenÃ© Pipeline pro dlouhÃ© pÅ™Ã­bÄ›hy

**ğŸŸ¢ Level 1 â€” LokÃ¡lnÃ­ (bez cloudu)**
```
Qwen 3 72B â†’ moving window â†’ story bible â†’ external summarizer
```
*VÃ½sledek: KompletnÃ­ romÃ¡n*

**ğŸŸ£ Level 2 â€” Hybrid (nejlepÅ¡Ã­ cena/kvalita)**
```
1. Qwen 2.5-32B EN nebo Mistral 12B: draft
2. GPT-5.1 / Sonnet: review + rewrite
3. Czech translation (if needed)
4. Final polish GPT-5.1
```
*Funguje SKVÄšLE pro PrismQ workflow*

**ğŸ”µ Level 3 â€” NejvyÅ¡Å¡Ã­ kvalita**
```
1. GPT-5.1: outline + story bible
2. Qwen 3 32B: long draft generation
3. GPT-5.1: deep literary rewrite
4. GPT-5.1: final author-level polish
```
*VÃ½sledek: Kvalita profesionÃ¡lnÃ­ povÃ­dkovÃ© tvorby*

#### ğŸ’» PrismQ Moving Window Implementation

```python
from T.Publishing.SEO.Keywords import AIConfig

class MovingWindowGenerator:
    """Moving Window generator pro dlouhÃ© pÅ™Ã­bÄ›hy."""
    
    def __init__(self, model: str = "qwen3:32b"):
        self.config = AIConfig(
            model=model,
            api_base="http://localhost:11434",
            temperature=0.75,
            enable_ai=True
        )
        self.story_bible = ""
        self.facts = []
        self.summaries = []
    
    def set_story_bible(self, bible: str):
        """NastavÃ­ Story Bible pro celÃ½ pÅ™Ã­bÄ›h."""
        self.story_bible = bible
    
    def generate_block(self, directive: str, window_size: int = 1200):
        """Generuje jeden blok pÅ™Ã­bÄ›hu."""
        context = self._build_context(directive)
        # Generate with context
        return self._call_model(context, window_size)
    
    def _build_context(self, directive: str) -> str:
        """SestavÃ­ kontext pro generovÃ¡nÃ­."""
        recent_summaries = self.summaries[-3:]  # PoslednÃ­ 3 shrnutÃ­
        recent_facts = self.facts[-10:]  # PoslednÃ­ch 10 faktÅ¯
        
        return f'''
Story Bible:
{self.story_bible}

Recent Summaries:
{chr(10).join(recent_summaries)}

Story Facts:
{chr(10).join(recent_facts)}

Directive:
{directive}

Now continue the story:
'''

# PouÅ¾itÃ­:
generator = MovingWindowGenerator("qwen3:32b")
generator.set_story_bible("""
Characters: Clara (28, melancholic), Evelyn (ghost, mysterious)
Setting: Willow Creek, haunted Victorian house
Tone: Gothic, suspenseful, emotional
Rules: No explicit violence, slow burn mystery
""")

# GenerovÃ¡nÃ­ blokÅ¯
block1 = generator.generate_block("Introduce Clara arriving at the house")
# ... summarize, extract facts, continue
```

#### ğŸ“‹ Checklist pro Moving Window

- [ ] Story Bible pÅ™ipraven
- [ ] Outline hotovÃ½ (10â€“18 beats)
- [ ] Window size nastaven podle modelu
- [ ] Summarizer nakonfigurovÃ¡n
- [ ] Fact extractor pÅ™ipraven
- [ ] Directive template vytvoÅ™en

### ğŸ”— Integrace Moving Window do PrismQ Workflow

Moving Window technika se hodÃ­ do **specifickÃ½ch krokÅ¯** PrismQ pipeline:

#### Kde pouÅ¾Ã­t Moving Window v PrismQ.T

| Stage | Moving Window? | DÅ¯vod |
|-------|----------------|-------|
| **PrismQ.T.Idea.From.User** | âŒ Ne | KrÃ¡tkÃ½ vÃ½stup (koncept) |
| **PrismQ.T.Story.From.Idea** | âš ï¸ VolitelnÄ› | Pro detailnÄ›jÅ¡Ã­ Story Bible |
| **PrismQ.T.Title.From.Idea** | âŒ Ne | KrÃ¡tkÃ½ vÃ½stup (titulky) |
| **PrismQ.T.Content.From.Title.Idea** | âœ… **ANO** | ğŸ† **HLAVNÃ USE CASE** |
| **PrismQ.T.Content.From.Title.Review.Script** | âœ… **ANO** | Refinement dlouhÃ©ho scriptu |
| **PrismQ.T.Story.Polish** | âœ… **ANO** | FinÃ¡lnÃ­ polish dlouhÃ©ho textu |
| Review stages | âŒ Ne | AnalytickÃ©, ne generativnÃ­ |

#### ğŸ† PrimÃ¡rnÃ­ integrace: Script Generation

```
PrismQ.T.Content.From.Title.Idea
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“– MOVING WINDOW INTEGRATION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1. NaÄti Idea + Title jako "Story Bible"            â”‚
â”‚ 2. Vygeneruj Outline (10-18 beats) z Idea           â”‚
â”‚ 3. Pro kaÅ¾dÃ½ beat:                                  â”‚
â”‚    a) Generate Block (300-500 slov)                 â”‚
â”‚    b) Summarize Block                               â”‚
â”‚    c) Extract Facts                                 â”‚
â”‚    d) Prepare Directive pro dalÅ¡Ã­ beat              â”‚
â”‚    e) Reinjekce context + continue                  â”‚
â”‚ 4. SpojenÃ­ blokÅ¯ â†’ Script v1                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
PrismQ.T.Review.Title.From.Script.Idea
```

#### NÃ¡vrh integrace do PrismQ.T.Content

**NovÃ½ modul:** `T/Content/From/Idea/Title/MovingWindow/`

```python
# T/Content/From/Idea/Title/MovingWindow/generator.py

from dataclasses import dataclass
from typing import List, Optional
import ollama

@dataclass
class StoryBeat:
    """Jeden beat (segment) pÅ™Ã­bÄ›hu."""
    number: int
    directive: str
    content: str = ""
    summary: str = ""
    facts: List[str] = None

class PrismQMovingWindowScript:
    """
    Moving Window Script Generator pro PrismQ.T.Content.From.Title.Idea
    
    Integrace do workflow:
    - Input: Idea + Title z pÅ™edchozÃ­ch stages
    - Output: Script v1 (dlouhÃ½ text bez memory drift)
    """
    
    def __init__(
        self, 
        model: str = "qwen3:32b",
        window_size: int = 1200,  # tokens
        block_words: int = 400    # target words per block
    ):
        self.model = model
        self.window_size = window_size
        self.block_words = block_words
        self.story_bible = ""
        self.outline: List[StoryBeat] = []
        self.generated_blocks: List[str] = []
        self.summaries: List[str] = []
        self.facts: List[str] = []
    
    def from_idea_and_title(self, idea: dict, title: str) -> str:
        """
        HlavnÃ­ entry point pro PrismQ.T.Content.From.Title.Idea
        
        Args:
            idea: Idea objekt z PrismQ.T.Idea.From.User
            title: Title z PrismQ.T.Title.From.Idea
            
        Returns:
            KompletnÃ­ Script v1
        """
        # 1. Build Story Bible from Idea
        self.story_bible = self._build_story_bible(idea, title)
        
        # 2. Generate Outline (10-18 beats)
        self.outline = self._generate_outline(idea, title)
        
        # 3. Generate each beat using Moving Window
        for beat in self.outline:
            block = self._generate_block(beat)
            self.generated_blocks.append(block)
            
            # Summarize and extract facts
            summary = self._summarize_block(block)
            self.summaries.append(summary)
            
            facts = self._extract_facts(block)
            self.facts.extend(facts)
        
        # 4. Combine into final script
        return self._combine_script()
    
    def _build_story_bible(self, idea: dict, title: str) -> str:
        """VytvoÅ™Ã­ Story Bible z Idea a Title."""
        return f'''
# Story Bible

## Title
{title}

## Core Concept
{idea.get("concept", "")}

## Target Audience
{idea.get("audience", "Teen/YA, 10-20, US women")}

## Tone
{idea.get("tone", "Engaging, emotional, relatable")}

## Characters
{idea.get("characters", "")}

## Setting
{idea.get("setting", "")}

## Rules
- No explicit content
- Consistent voice throughout
- Relatable situations for target audience
- Clear emotional arc
'''
    
    def _generate_outline(self, idea: dict, title: str) -> List[StoryBeat]:
        """Generuje 10-18 beat outline."""
        prompt = f'''Based on this idea and title, create a detailed story outline.

Title: {title}
Concept: {idea.get("concept", "")}

Write exactly 12 beats (story segments) that form a complete narrative arc:
- Beat 1-2: Setup and introduction
- Beat 3-4: Rising action begins
- Beat 5-6: Complications
- Beat 7-8: Midpoint twist
- Beat 9-10: Escalation
- Beat 11: Climax
- Beat 12: Resolution

For each beat, provide a 1-2 sentence directive of what should happen.
'''
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}]
        )
        
        # Parse response into StoryBeat objects
        return self._parse_outline(response["message"]["content"])
    
    def _generate_block(self, beat: StoryBeat) -> str:
        """Generuje jeden blok pomocÃ­ Moving Window."""
        # Build context from recent summaries and facts
        recent_context = self._build_recent_context()
        
        prompt = f'''
{self.story_bible}

## Previous Context
{recent_context}

## Current Directive (Beat {beat.number})
{beat.directive}

## Instructions
Write the next segment of the story (~{self.block_words} words).
- Continue naturally from previous context
- Follow the directive
- Maintain consistent voice and tone
- End at a natural pause point

Now continue the story:
'''
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            options={"num_predict": self.window_size}
        )
        
        return response["message"]["content"]
    
    def _build_recent_context(self) -> str:
        """SestavÃ­ kontext z poslednÃ­ch shrnutÃ­ a faktÅ¯."""
        recent_summaries = self.summaries[-3:] if self.summaries else []
        recent_facts = self.facts[-15:] if self.facts else []
        
        context = ""
        if recent_summaries:
            context += "### Recent Summaries\n"
            context += "\n".join(recent_summaries)
            context += "\n\n"
        
        if recent_facts:
            context += "### Story Facts So Far\n"
            context += "\n".join(f"- {fact}" for fact in recent_facts)
        
        return context if context else "This is the beginning of the story."
    
    def _summarize_block(self, block: str) -> str:
        """VytvoÅ™Ã­ shrnutÃ­ bloku."""
        prompt = f'''Summarize this story segment in 2-3 sentences:

{block}

Summary:'''
        
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            options={"num_predict": 200}
        )
        
        return response["message"]["content"]
    
    def _extract_facts(self, block: str) -> List[str]:
        """Extrahuje klÃ­ÄovÃ¡ fakta z bloku."""
        prompt = f'''Extract 3-5 key facts from this story segment.
Format as simple bullet points.

{block}

Facts:'''
        
        response = ollama.chat(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            options={"num_predict": 300}
        )
        
        # Parse bullet points
        lines = response["message"]["content"].strip().split("\n")
        return [line.strip("- ").strip() for line in lines if line.strip()]
    
    def _combine_script(self) -> str:
        """SpojÃ­ vÅ¡echny bloky do finÃ¡lnÃ­ho scriptu."""
        return "\n\n".join(self.generated_blocks)
    
    def _parse_outline(self, outline_text: str) -> List[StoryBeat]:
        """Parsuje outline text do StoryBeat objektÅ¯."""
        beats = []
        lines = outline_text.strip().split("\n")
        beat_num = 0
        
        for line in lines:
            line = line.strip()
            if line and any(line.startswith(f"Beat {i}") or line.startswith(f"{i}.") or line.startswith(f"{i}:") for i in range(1, 19)):
                beat_num += 1
                directive = line.split(":", 1)[-1].strip() if ":" in line else line
                beats.append(StoryBeat(number=beat_num, directive=directive))
        
        return beats if beats else [StoryBeat(number=1, directive="Write the story")]


# Integrace do PrismQ workflow
def script_from_idea_title_moving_window(idea: dict, title: str) -> str:
    """
    Entry point pro PrismQ.T.Content.From.Title.Idea s Moving Window.
    
    PouÅ¾itÃ­:
        from T.Script.From.Idea.Title.MovingWindow import script_from_idea_title_moving_window
        
        script = script_from_idea_title_moving_window(idea, title)
    """
    generator = PrismQMovingWindowScript(
        model="qwen3:32b",
        window_size=1200,
        block_words=400
    )
    return generator.from_idea_and_title(idea, title)
```

#### Konfigurace pro rÅ¯znÃ© dÃ©lky scriptu

| DÃ©lka scriptu | PoÄet beats | Block size | Celkem slov |
|---------------|-------------|------------|-------------|
| **KrÃ¡tkÃ½** (Reddit story) | 5-8 | 300 slov | 1,500-2,400 |
| **StÅ™ednÃ­** (Short story) | 10-12 | 400 slov | 4,000-4,800 |
| **DlouhÃ½** (Novella) | 15-20 | 500 slov | 7,500-10,000 |
| **Velmi dlouhÃ½** (Novel chapter) | 20-30 | 600 slov | 12,000-18,000 |

#### DoporuÄenÃ½ workflow s Moving Window

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PrismQ.T Pipeline                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Idea.From.User                                           â”‚
â”‚       â†“                                                     â”‚
â”‚  2. Story.From.Idea (creates Story Bible)                   â”‚
â”‚       â†“                                                     â”‚
â”‚  3. Title.From.Idea                                         â”‚
â”‚       â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 4. Script.From.Title.Idea                            â”‚   â”‚
â”‚  â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚   â”‚
â”‚  â”‚    â”‚ ğŸ“– MOVING WINDOW GENERATOR                  â”‚   â”‚   â”‚
â”‚  â”‚    â”‚                                             â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Story Bible â† Idea + Title               â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Outline (12 beats)                       â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ For each beat:                           â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Generate Block                         â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Summarize                              â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Extract Facts                          â”‚   â”‚   â”‚
â”‚  â”‚    â”‚    â†’ Prepare Directive                      â”‚   â”‚   â”‚
â”‚  â”‚    â”‚  â€¢ Combine â†’ Script v1                      â”‚   â”‚   â”‚
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â†“                                                     â”‚
â”‚  5. Review.Title.From.Script.Idea                             â”‚
â”‚       â†“                                                     â”‚
â”‚  6-16. Quality Reviews (Grammar, Tone, Content...)          â”‚
â”‚       â†“                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ 17-18. Story.Review + Story.Polish                   â”‚   â”‚
â”‚  â”‚    (Moving Window pro Polish refinement)             â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚       â†“                                                     â”‚
â”‚  Publishing                                                 â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Kdy pouÅ¾Ã­t Moving Window vs Standard Generation

| Situace | Metoda | DÅ¯vod |
|---------|--------|-------|
| Reddit Stories (<2000 slov) | Standard | RychlejÅ¡Ã­, kontext staÄÃ­ |
| Short Stories (2000-5000 slov) | Moving Window | Prevence memory drift |
| Novellas (5000-15000 slov) | Moving Window | **PovinnÃ©** |
| Novel chapters (15000+ slov) | Moving Window | **PovinnÃ©** |
| Title generation | Standard | KrÃ¡tkÃ½ vÃ½stup |
| Reviews | Standard | AnalytickÃ©, ne generativnÃ­ |
| Script refinement | Moving Window | ZachovÃ¡nÃ­ konzistence |

For RTX 5090 with 32GB VRAM, we recommend:

```bash
# Best quality for story writing
ollama pull qwen3:32b

# Alternative for SEO and metadata
ollama pull llama3.1:70b-q4_K_M
```

**PrismQ Configuration for RTX 5090:**

```python
from T.Publishing.SEO.Keywords import AIConfig

# High-quality story generation config
story_config = AIConfig(
    model="qwen3:32b",
    api_base="http://localhost:11434",
    temperature=0.7,  # Higher for creative writing
    max_tokens=2000,
    enable_ai=True
)

# SEO metadata config
seo_config = AIConfig(
    model="llama3.1:70b-q4_K_M",
    api_base="http://localhost:11434",
    temperature=0.3,  # Lower for consistent SEO output
    enable_ai=True
)
```

### Optimalizace naÄÃ­tÃ¡nÃ­ modelu (Model Loading Optimization)

Pro zamezenÃ­ opakovanÃ©ho naÄÃ­tÃ¡nÃ­ modelu do VRAM bÄ›hem bÄ›hu PrismQ:

#### Ollama Keep-Alive nastavenÃ­

Ollama standardnÄ› udrÅ¾uje model v pamÄ›ti 5 minut po poslednÃ­m dotazu. Pro delÅ¡Ã­ workflow:

```bash
# Nastavte OLLAMA_KEEP_ALIVE na delÅ¡Ã­ dobu (napÅ™. 60 minut)
export OLLAMA_KEEP_ALIVE=60m

# Nebo permanentnÄ› v .bashrc / .zshrc
echo 'export OLLAMA_KEEP_ALIVE=60m' >> ~/.bashrc
```

**Windows (PowerShell):**
```powershell
# Nastavte promÄ›nnou prostÅ™edÃ­
$env:OLLAMA_KEEP_ALIVE = "60m"

# Nebo permanentnÄ›
[System.Environment]::SetEnvironmentVariable("OLLAMA_KEEP_ALIVE", "60m", "User")
```

#### PrismQ Model Manager (doporuÄenÃ½ pÅ™Ã­stup)

Pro optimÃ¡lnÃ­ vÃ½kon pouÅ¾ijte jednotnÃ½ model pro celÃ½ workflow:

```python
import ollama

class PrismQModelManager:
    """SprÃ¡vce modelu pro efektivnÃ­ vyuÅ¾itÃ­ VRAM."""
    
    _instance = None
    _current_model = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def ensure_model_loaded(self, model_name: str):
        """NaÄte model pouze pokud jeÅ¡tÄ› nenÃ­ v pamÄ›ti."""
        if self._current_model != model_name:
            # Warmup dotaz pro naÄtenÃ­ modelu do VRAM
            ollama.chat(
                model=model_name,
                messages=[{"role": "user", "content": "Hello"}],
                options={"num_predict": 1}
            )
            self._current_model = model_name
            print(f"Model {model_name} naÄten do VRAM")
        return self._current_model

# PouÅ¾itÃ­ na zaÄÃ¡tku workflow
manager = PrismQModelManager.get_instance()
manager.ensure_model_loaded("qwen3:32b")  # Primary local model

# VÅ¡echny nÃ¡sledujÃ­cÃ­ dotazy pouÅ¾ijÃ­ jiÅ¾ naÄtenÃ½ model
```

#### DoporuÄenÃ¡ strategie pro celÃ½ PrismQ workflow

| FÃ¡ze | Model | DÅ¯vod |
|------|-------|-------|
| **Idea â†’ Title â†’ Script â†’ Review** | `qwen3:32b` | ğŸ† Jeden model pro celÃ½ workflow, bez pÅ™epÃ­nÃ¡nÃ­ |
| **SEO Metadata** (volitelnÄ›) | `qwen3:32b` | KonzistentnÃ­ vÃ½stupy |

> **Tip:** Pro maximÃ¡lnÃ­ efektivitu pouÅ¾Ã­vejte jeden model (Qwen 3:30B) pro celÃ½ bÄ›h. PÅ™epÃ­nÃ¡nÃ­ mezi modely vyÅ¾aduje uvolnÄ›nÃ­ a naÄtenÃ­ ~20-40GB dat, coÅ¾ trvÃ¡ 10-30 sekund.

### OptimÃ¡lnÃ­ konfigurace pro Ryzen 9 9900X3D + RTX 5090

Pro vÃ¡Å¡ konkrÃ©tnÃ­ hardware (AMD Ryzen 9 9900X3D + RTX 5090 32GB):

| Parametr | DoporuÄenÃ¡ hodnota | DÅ¯vod |
|----------|-------------------|-------|
| **Model** | `qwen3:32b` | ğŸ† Primary local model, plnÄ› vyuÅ¾ije RTX 5090 |
| **VRAM Usage** | ~19GB | Dostatek prostoru pro context |
| **Context Length** | 8192-32768 | VyuÅ¾ije 3D V-Cache pro KV cache |
| **Batch Size** | 1 | StandardnÃ­ pro generovÃ¡nÃ­ |
| **GPU Layers** | All (auto) | CelÃ½ model na GPU |

```bash
# OptimÃ¡lnÃ­ Ollama konfigurace pro Ryzen 9 9900X3D + RTX 5090
export OLLAMA_NUM_PARALLEL=1          # Jeden request najednou
export OLLAMA_KEEP_ALIVE=60m          # Model zÅ¯stane v pamÄ›ti
export OLLAMA_MAX_LOADED_MODELS=1     # Jeden model najednou (Å¡etÅ™Ã­ VRAM)

# SpusÅ¥te Ollama
ollama serve
```

**VyuÅ¾itÃ­ 3D V-Cache (141MB):**
Ryzen 9 9900X3D mÃ¡ masivnÃ­ L3 cache, kterÃ¡ pomÃ¡hÃ¡ s:
- RychlejÅ¡Ã­m tokenizaÄnÃ­m pre/post-processingem
- EfektivnÄ›jÅ¡Ã­m CPU offloadingem (pokud potÅ™ebnÃ½)
- NiÅ¾Å¡Ã­ latencÃ­ pÅ™i komunikaci s GPU
```

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "mosaicml/mpt-7b-storywriter"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

prompt = "Write the opening chapter of a dark horror story set in a small Czech town:"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=1000,
    temperature=0.7,
    do_sample=True
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

> **Note**: MPT-7B-StoryWriter requires manual setup with HuggingFace Transformers and is not directly supported by Ollama. For easier integration with PrismQ, we recommend Qwen2.5:32b.

## Step 3 â€“ Test the Model

After the download finishes, verify the installation:

```bash
ollama run qwen3:32b
```

You'll enter an interactive prompt. Try a test query:

```
Write a dark, emotional horror story opening set in a small Czech town at night.
```

If it responds with a story, Qwen 3:30B is correctly installed. Type `/bye` to exit.

## Step 4 â€“ Python Integration

### Install the Ollama Python Package

```bash
pip install ollama
```

### Minimal Python Script

Create a file named `story_test.py`:

```python
import ollama

prompt = "Write the first 500 words of a psychological horror story told in first person."

response = ollama.chat(
    model="qwen3:32b",  # Primary local AI model
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response["message"]["content"])
```

Run it:

```bash
python story_test.py
```

This directly uses your local Qwen 3:30B via Ollama's HTTP API on `localhost:11434`.

## PrismQ Integration

### Using AI in SEO Metadata Generation

PrismQ's SEO Keywords module already supports Ollama integration. Example usage:

```python
from T.Publishing.SEO.Keywords import process_content_seo, AIConfig

# Configure to use Qwen 3:30B (primary local model)
config = AIConfig(
    model="qwen3:32b",
    api_base="http://localhost:11434",
    temperature=0.3,
    enable_ai=True
)

result = process_content_seo(
    title="Your Content Title",
    script="Your content script...",
    use_ai=True,
    ai_config=config,
    brand_name="Your Brand"
)

print(result['meta_description'])
print(result['title_tag'])
```

### Default AI Configuration

The default configuration in PrismQ varies by module:
- **SEO Keywords Module**: Uses `llama3.1:70b-q4_K_M` (optimized for SEO tasks)
- **General Content Generation**: Recommended `qwen2.5:14b` (best for creative writing)

Common settings:
- **API Base**: `http://localhost:11434`
- **Temperature**: 0.3 (lower for more consistent output)

You can customize these settings using the `AIConfig` class to match your model choice.

## Troubleshooting

### Ollama Not Running

If you see errors about Ollama being unavailable:

1. **Windows**: Check if Ollama is running in the system tray
2. **macOS**: Launch Ollama from Applications
3. **Linux**: Start the service with `ollama serve`

### Model Not Found

If a model is not found, pull it first:

```bash
ollama pull qwen2.5:14b
```

### Out of Memory Errors

If you encounter memory errors:
- Use a smaller model variant (e.g., `qwen2.5:7b` instead of `qwen2.5:14b`)
- Close other GPU-intensive applications
- Increase system swap space

### Slow Response Times

For faster inference:
- Ensure you're using GPU acceleration (NVIDIA CUDA or Apple Metal)
- Consider using quantized model variants
- Reduce `max_tokens` in the configuration

## API Reference

Ollama exposes a REST API at `http://localhost:11434`. Key endpoints:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/generate` | POST | Generate text completion |
| `/api/chat` | POST | Chat-style completion |
| `/api/tags` | GET | List available models |
| `/api/pull` | POST | Download a model |

### Example API Call

```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "qwen2.5:14b",
        "prompt": "Write a short story about...",
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 500
        }
    },
    timeout=30
)

result = response.json()
print(result["response"])
```

---

## ğŸš€ After-Completion Feature / Future Enhancement

> **PoznÃ¡mka od GPT:** NÃ¡sledujÃ­cÃ­ sekce popisuje budoucÃ­ rozÅ¡Ã­Å™enÃ­ PrismQ systÃ©mu, kterÃ¡ navazujÃ­ na Moving Window techniku dokumentovanou vÃ½Å¡e. Tyto funkce jsou navrÅ¾eny jako nadstavba souÄasnÃ© architektury bez breaking changes.

### 1) Moving-Window Engine pro dlouhÃ© generovÃ¡nÃ­ skriptÅ¯

Do budoucna lze nad textovÃ½m modulem PrismQ (`T â†’ Script`) postavit specializovanÃ½ **moving-window engine**, kterÃ½ bude generovat pÅ™Ã­bÄ›hy nebo scÃ©nÃ¡Å™e po blocÃ­ch (300â€“600 slov) mÃ­sto jednorÃ¡zovÃ©ho dlouhÃ©ho vÃ½stupu. 

**KlÃ­ÄovÃ© vlastnosti:**
- VyuÅ¾Ã­vÃ¡ **outline** a **story bible** vytvoÅ™enÃ© silnÃ½m modelem (GPT-5.1 / Sonnet)
- Vede lokÃ¡lnÃ­ modely (Qwen / Mistral) pÅ™es sekvenÄnÃ­ generovÃ¡nÃ­
- Proces: shrnutÃ­ â†’ extrakce faktÅ¯ â†’ plÃ¡novÃ¡nÃ­ dalÅ¡Ã­ho dÄ›je
- **VÃ½sledek:** LokÃ¡lnÃ­ modely produkujÃ­ delÅ¡Ã­ a kvalitnÄ›jÅ¡Ã­ text bez ztrÃ¡ty konzistence

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           MOVING-WINDOW ENGINE (Future)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  GPT-5.1/Sonnet                                             â”‚
â”‚       â”‚                                                     â”‚
â”‚       â”œâ”€â”€ Outline (10-18 beats)                             â”‚
â”‚       â””â”€â”€ Story Bible                                       â”‚
â”‚              â”‚                                              â”‚
â”‚              â–¼                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  Local Model (Qwen/Mistral) - Moving Window Loop    â”‚   â”‚
â”‚  â”‚                                                      â”‚   â”‚
â”‚  â”‚  Block 1 â†’ Summary â†’ Facts â†’ Directive â†’ Block 2    â”‚   â”‚
â”‚  â”‚  Block 2 â†’ Summary â†’ Facts â†’ Directive â†’ Block 3    â”‚   â”‚
â”‚  â”‚  ...                                                 â”‚   â”‚
â”‚  â”‚  Block N â†’ Final Script                              â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2) Story Bible, Block Memory a Directives jako novÃ© datovÃ© vrstvy

Do struktury PrismQ lze doplnit volitelnÃ© budoucÃ­ objekty, kterÃ© budou tvoÅ™it **persistentnÃ­ "pamÄ›Å¥" pÅ™Ã­bÄ›hu**:

| Objekt | Popis | UklÃ¡dÃ¡nÃ­ |
|--------|-------|----------|
| **StoryOutline** | Kostra pÅ™Ã­bÄ›hu (10-18 beats) | `T/{id}/Text/outline.json` |
| **StoryBible** | Postavy, pravidla, tone guide | `T/{id}/Text/bible.json` |
| **StoryBlock** | JednotlivÃ© bloky textu | `T/{id}/Text/blocks/` |
| **BlockSummary** | ShrnutÃ­ kaÅ¾dÃ©ho bloku | `T/{id}/Text/blocks/{n}/summary.txt` |
| **BlockFacts** | ExtrahovanÃ¡ fakta | `T/{id}/Text/blocks/{n}/facts.json` |
| **BlockDirective** | Instrukce pro dalÅ¡Ã­ blok | `T/{id}/Text/blocks/{n}/directive.txt` |

**UklÃ¡dÃ¡nÃ­ a Å™Ã­zenÃ­:**
- Artefakty uklÃ¡dÃ¡ny do `T/{id}/Text/...`
- MetadatovÄ› Å™Ã­zeny pÅ™es SQLite (`db.s3db`)
- UmoÅ¾Åˆuje sledovat a Å™Ã­dit dlouhodobou kontinuitu textu

**Integrace:**
```python
# BudoucÃ­ datovÃ¡ struktura
class StoryMemory:
    outline: StoryOutline        # 10-18 beats
    bible: StoryBible            # Characters, rules, tone
    blocks: List[StoryBlock]     # Generated blocks
    
class StoryBlock:
    number: int
    content: str                 # 300-600 slov
    summary: str                 # KondenzovanÃ¡ pamÄ›Å¥
    facts: List[str]             # KlÃ­ÄovÃ¡ fakta
    directive: str               # Instrukce pro dalÅ¡Ã­ blok
```

> **DÅ¯leÅ¾itÃ©:** PÅ™idÃ¡nÃ­ tÄ›chto objektÅ¯ rozÅ¡Ã­Å™Ã­ PrismQ o moÅ¾nost sledovat a Å™Ã­dit dlouhodobou kontinuitu textu, **aniÅ¾ by se mÄ›nila souÄasnÃ¡ architektura** â€“ jde o nadstavbu, ne o zÃ¡sah do existujÃ­cÃ­ho workflow.

### 3) Orchestrace blokovÃ©ho psanÃ­ jako volitelnÃ½ Script-Draft mÃ³d

Moving-window systÃ©m mÅ¯Å¾e bÃ½t v budoucnu zaveden jako **alternativnÃ­ nebo pokroÄilÃ½ reÅ¾im** pro fÃ¡zi `T.Script`:

**Kdy se aktivuje:**
- UÅ¾ivatel chce generovat dlouhÃ© scÃ©nÃ¡Å™e (5000+ slov)
- PovÃ­dky nebo podcastovÃ© epizody
- Serialized content (sÃ©rie)

**ZachovÃ¡nÃ­ kompatibility:**
```
SouÄasnÃ© workflow (beze zmÄ›n):
    Idea â†’ Title â†’ Script â†’ Reviews â†’ Refinements

NovÃ½ "Loop Mode" (volitelnÃ½):
    Idea â†’ Title â†’ Script[Loop Mode] â†’ Reviews â†’ Refinements
                        â”‚
                        â”œâ”€â”€ Block 1 â†’ Summary â†’ Facts
                        â”œâ”€â”€ Block 2 â†’ Summary â†’ Facts
                        â”œâ”€â”€ Block 3 â†’ Summary â†’ Facts
                        â””â”€â”€ ... â†’ Final Script
```

**KlÃ­ÄovÃ© vlastnosti Loop Mode:**
- SekvenÄnÄ› orchestruje: **generovÃ¡nÃ­ bloku â†’ shrnutÃ­ â†’ fakta â†’ direktiva â†’ dalÅ¡Ã­ blok**
- Aktivuje se pouze na vyÅ¾Ã¡dÃ¡nÃ­
- **Å½Ã¡dnÃ© breaking changes** v souÄasnÃ©m workflow
- UmoÅ¾nÃ­ PrismQ rÅ¯st smÄ›rem k robustnÃ­mu systÃ©movÃ©mu psanÃ­ delÅ¡Ã­ch textÅ¯

**NavrÅ¾enÃ½ API interface:**

```python
# SouÄasnÃ½ zpÅ¯sob (zachovÃ¡n)
script = Script.from_title_idea(title, idea)

# NovÃ½ Loop Mode (budoucÃ­ rozÅ¡Ã­Å™enÃ­)
script = Script.from_title_idea(
    title, 
    idea,
    mode="loop",           # Aktivuje Moving Window
    block_size=400,        # Slov na blok
    target_length=5000     # CelkovÃ¡ dÃ©lka
)
```

### 4) Integrating Iterative Text Refinement in the PrismQ Pipeline

> **Research Summary:** Tento vÃ½zkum popisuje, jak implementovat iterativnÃ­ vylepÅ¡ovÃ¡nÃ­ textu v PrismQ pÅ™i respektovÃ¡nÃ­ SOLID principÅ¯.

#### ğŸ¯ ZÃ¡kladnÃ­ principy

**StriktnÃ­ oddÄ›lenÃ­ odpovÄ›dnostÃ­:**

| Typ modulu | PÅ™Ã­klady | Funkce |
|------------|----------|--------|
| **Generation/Rewrite** | `Script.From.Title.Idea`, `Title.From.Script.Review.Title` | ProdukujÃ­ nebo pÅ™episujÃ­ text |
| **Review** | `Review.Script.From.Title`, `Review.Title.From.Script.Idea` | Pouze analyzujÃ­, nikdy nemÄ›nÃ­ text |

**SOLID iterativnÃ­ pipeline:**
```
Review = DiagnÃ³za (score, issues, suggestions)
Rewrite = LÃ©Äba (aplikace zmÄ›n na zÃ¡kladÄ› review)
```

#### ğŸ“ Kde pouÅ¾Ã­t iterativnÃ­ refinement v PrismQ

**2.1 Idea / Concept Generation**
```
Idea.Inspiration / Idea.Fusion / Idea.From.User
    â†“
Review (clarity, story arc potential, brand fit)
    â†“
Refine chosen idea (optional)
```

**2.2 Title Generation**
```
Title.From.Idea
    â†“
Review.Title.From.Script.Idea (SEO, VO-friendliness, tone)
    â†“
Title.From.Script.Review.Title (refinement loop)
```

**2.3 Script / Outline Generation** (hlavnÃ­ use case)
```
Script.From.Title.Idea
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ITERATIVE REFINEMENT LOOP             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  1. Review.Script.Grammar              â”‚
â”‚  2. Review.Script.Readability          â”‚
â”‚  3. Review.Script.Tone                 â”‚
â”‚  4. Review.Script.VO.Friendliness      â”‚
â”‚  5. Review.Script.Consistency          â”‚
â”‚       â†“                                â”‚
â”‚  Script.From.Title.Review.Script       â”‚
â”‚  (aplicuje zmÄ›ny z reviews)            â”‚
â”‚       â†“                                â”‚
â”‚  Loop until all reviews pass           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Final Script
```

**2.4 Voiceover Text Refinement**
```
Script (raw)
    â†“
Review.Script.VO.Friendliness
    - Pacing pro ÄtenÃ­ nahlas
    - Sentence length pro breathing
    - Pronunciation issues
    â†“
Script.VO.Polish
    â†“
Final VO-ready text
```

#### ğŸ”„ IterativnÃ­ smyÄka - Implementace

```python
# PÅ™Ã­klad iterativnÃ­ho refinement v PrismQ
class IterativeScriptRefinement:
    """
    IterativnÃ­ refinement pro Script s SOLID principy.
    
    Review moduly = pouze diagnÃ³za
    Rewrite moduly = pouze lÃ©Äba
    """
    
    MAX_ITERATIONS = 5
    QUALITY_THRESHOLD = 8.0  # Minimum score pro pass
    
    def __init__(self, script: str, title: str, idea: dict):
        self.script = script
        self.title = title
        self.idea = idea
        self.iteration = 0
        self.reviews = []
    
    def refine(self) -> str:
        """HlavnÃ­ refinement loop."""
        while self.iteration < self.MAX_ITERATIONS:
            self.iteration += 1
            
            # 1. REVIEW PHASE (diagnosis only)
            review_results = self._run_all_reviews()
            self.reviews.append(review_results)
            
            # 2. CHECK IF PASSED
            if self._all_reviews_passed(review_results):
                return self.script  # âœ… Quality threshold reached
            
            # 3. REWRITE PHASE (treatment)
            self.script = self._apply_refinements(review_results)
        
        return self.script  # Max iterations reached
    
    def _run_all_reviews(self) -> dict:
        """SpustÃ­ vÅ¡echny review moduly (Å¾Ã¡dnÃ© zmÄ›ny textu)."""
        return {
            "grammar": Review.Script.Grammar(self.script),
            "readability": Review.Script.Readability(self.script),
            "tone": Review.Script.Tone(self.script, self.idea),
            "vo_friendliness": Review.Script.VO.Friendliness(self.script),
            "consistency": Review.Script.Consistency(self.script, self.idea),
        }
    
    def _all_reviews_passed(self, reviews: dict) -> bool:
        """Kontrola, zda vÅ¡echny reviews proÅ¡ly."""
        return all(
            review.score >= self.QUALITY_THRESHOLD 
            for review in reviews.values()
        )
    
    def _apply_refinements(self, reviews: dict) -> str:
        """Aplikuje refinement na zÃ¡kladÄ› review vÃ½sledkÅ¯."""
        # Agreguje issues ze vÅ¡ech reviews
        all_issues = []
        for review in reviews.values():
            all_issues.extend(review.issues)
        
        # VolÃ¡ rewrite modul s issues
        return Script.From.Title.Review.Script(
            script=self.script,
            title=self.title,
            issues=all_issues,
            suggestions=[r.suggestions for r in reviews.values()]
        )
```

#### ğŸ“Š Review Module Interface

```python
@dataclass
class ReviewResult:
    """StandardnÃ­ vÃ½stup review modulu (SOLID - pouze diagnÃ³za)."""
    
    score: float           # 0.0 - 10.0
    passed: bool           # score >= threshold
    issues: List[str]      # Seznam nalezenÃ½ch problÃ©mÅ¯
    suggestions: List[str] # NÃ¡vrhy na zlepÅ¡enÃ­
    metrics: dict          # DetailnÃ­ metriky
    
    # Review NIKDY neobsahuje:
    # - modified_text
    # - new_text
    # - corrections

# PÅ™Ã­klad pouÅ¾itÃ­
grammar_review = Review.Script.Grammar(script)
# Returns:
# ReviewResult(
#     score=7.5,
#     passed=False,
#     issues=["Run-on sentence in paragraph 3", "Missing comma before 'and'"],
#     suggestions=["Split long sentences", "Add punctuation"],
#     metrics={"sentence_count": 45, "avg_length": 18.5}
# )
```

#### ğŸ›ï¸ Konfigurace iterativnÃ­ho refinement

```python
# Konfigurace pro rÅ¯znÃ© quality levels
REFINEMENT_CONFIGS = {
    "draft": {
        "max_iterations": 2,
        "quality_threshold": 6.0,
        "reviews": ["grammar", "readability"]
    },
    "standard": {
        "max_iterations": 3,
        "quality_threshold": 7.5,
        "reviews": ["grammar", "readability", "tone", "consistency"]
    },
    "premium": {
        "max_iterations": 5,
        "quality_threshold": 8.5,
        "reviews": ["grammar", "readability", "tone", "consistency", "vo_friendliness"]
    },
    "voiceover": {
        "max_iterations": 5,
        "quality_threshold": 9.0,
        "reviews": ["grammar", "readability", "vo_friendliness", "pacing", "pronunciation"]
    }
}
```

#### ğŸ”— Integrace s Moving Window

IterativnÃ­ refinement se kombinuje s Moving Window pro dlouhÃ© texty:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  COMBINED PIPELINE: Moving Window + Iterative Refinement   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  FOR EACH BLOCK:                                            â”‚
â”‚    1. Generate Block (300-500 words)                        â”‚
â”‚    2. â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚       â”‚  ITERATIVE REFINEMENT (per block)   â”‚              â”‚
â”‚       â”‚  - Review.Grammar                   â”‚              â”‚
â”‚       â”‚  - Review.Tone                      â”‚              â”‚
â”‚       â”‚  - Review.Consistency               â”‚              â”‚
â”‚       â”‚  â†’ Refine until pass                â”‚              â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚    3. Summarize Block                                       â”‚
â”‚    4. Extract Facts                                         â”‚
â”‚    5. Prepare Directive                                     â”‚
â”‚    6. Continue to next block                                â”‚
â”‚                                                             â”‚
â”‚  FINAL:                                                     â”‚
â”‚    - Combine all refined blocks                             â”‚
â”‚    - Final full-text review                                 â”‚
â”‚    - VO-friendliness pass                                   â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### ğŸ“‹ Checklist pro implementaci

- [ ] Standardizovat `ReviewResult` interface pro vÅ¡echny review moduly
- [ ] Review moduly nikdy nemÄ›nÃ­ text (SOLID)
- [ ] Rewrite moduly pÅ™ijÃ­majÃ­ issues + suggestions jako input
- [ ] Konfigurovat `max_iterations` a `quality_threshold` per use case
- [ ] Integrovat s Moving Window pro dlouhÃ© texty
- [ ] VO-friendliness jako finÃ¡lnÃ­ quality gate

### 5) Local AI Review Prompt for Qwen 3:30B

> **Recommended Model:** Qwen 3:30B for local AI generation and review tasks on RTX 5090.

This is the standard prompt for local AI story review using Qwen 3:30B. Use this prompt to get critical, actionable feedback on story drafts.

#### ğŸ“ Story Review Prompt Template

```
Write a critical review of the following story that focuses exclusively on its biggest flaws in structure, pacing, worldbuilding, logic, thematic execution, and character development.

Requirements:

Length: Maximum 1200 words. Do NOT exceed.

Tone: analytical, objective, constructive; avoid excessive praise.

Do NOT summarize the entire plot.

Focus your critique on:

Major pacing and narrative-flow issues

Worldbuilding inconsistencies or contradictions

Logical gaps in the story's rules or mechanics

Underdeveloped or unclear character motivations

Thematic weaknesses or missed opportunities

Structural problems that reduce emotional impact

Use specific examples from the story for each flaw.

Provide actionable suggestions explaining how the author can improve or fix each issue.

Avoid:

Superlatives

Unjustified praise

Invention of scenes not present in the text

Vague criticism without evidence

Structure your review as follows:

Introduction: brief statement of what the story attempts to accomplish

Major Flaws: bullet points or subsections with evidence

Suggestions for Improvement: clear and practical

Conclusion: short summary of why the weaknesses matter

Final Score: Give a numerical score 0â€“100% based on overall effectiveness in light of its flaws

Readiness Statement:

If the score is 75% or higher, explicitly state: "This story is ready for final polish."

If the score is below 75%, explicitly state: "This story is not yet ready for final polish."

Now analyze the following story:
[INSERT STORY HERE]
```

#### ğŸ”§ Usage in PrismQ

```python
import ollama

def review_story_local(story_text: str) -> dict:
    """
    Review a story using local Qwen 3:30B model.
    
    Args:
        story_text: The complete story text to review
        
    Returns:
        dict with review, score, and readiness status
    """
    
    REVIEW_PROMPT = """Write a critical review of the following story that focuses exclusively on its biggest flaws in structure, pacing, worldbuilding, logic, thematic execution, and character development.

Requirements:
- Length: Maximum 1200 words. Do NOT exceed.
- Tone: analytical, objective, constructive; avoid excessive praise.
- Do NOT summarize the entire plot.

Focus your critique on:
- Major pacing and narrative-flow issues
- Worldbuilding inconsistencies or contradictions
- Logical gaps in the story's rules or mechanics
- Underdeveloped or unclear character motivations
- Thematic weaknesses or missed opportunities
- Structural problems that reduce emotional impact

Use specific examples from the story for each flaw.
Provide actionable suggestions explaining how the author can improve or fix each issue.

Avoid:
- Superlatives
- Unjustified praise
- Invention of scenes not present in the text
- Vague criticism without evidence

Structure your review as follows:
1. Introduction: brief statement of what the story attempts to accomplish
2. Major Flaws: bullet points or subsections with evidence
3. Suggestions for Improvement: clear and practical
4. Conclusion: short summary of why the weaknesses matter
5. Final Score: Give a numerical score 0â€“100% based on overall effectiveness in light of its flaws

Readiness Statement:
- If the score is 75% or higher, explicitly state: "This story is ready for final polish."
- If the score is below 75%, explicitly state: "This story is not yet ready for final polish."

Now analyze the following story:
"""
    
    response = ollama.chat(
        model="qwen3:32b",  # Qwen 3:30B for local review
        messages=[
            {"role": "user", "content": REVIEW_PROMPT + story_text}
        ]
    )
    
    review_text = response["message"]["content"]
    
    # Parse score from review
    import re
    score_match = re.search(r'(\d+)%', review_text)
    score = int(score_match.group(1)) if score_match else 0
    
    return {
        "review": review_text,
        "score": score,
        "ready_for_polish": score >= 75,
        "model": "qwen3:32b"
    }
```

#### ğŸ“Š Integration with Iterative Refinement

This review prompt integrates with the iterative refinement pipeline:

```
Story Draft
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOCAL REVIEW (Qwen 3:30B)              â”‚
â”‚  - Critical analysis                     â”‚
â”‚  - Score 0-100%                          â”‚
â”‚  - Actionable suggestions                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Score >= 75%? â”€â”€â†’ YES â†’ "Ready for final polish"
    â”‚
    NO
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  REWRITE (apply suggestions)            â”‚
â”‚  - Fix major flaws                       â”‚
â”‚  - Address pacing issues                 â”‚
â”‚  - Strengthen character motivations      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
Loop back to LOCAL REVIEW
```

#### âš™ï¸ Ollama Setup for Qwen 3:30B

```bash
# Pull Qwen 3:30B model
ollama pull qwen3:32b

# Or with specific quantization for RTX 5090 (32GB VRAM)
ollama pull qwen3:32b-q4_K_M

# Set keep-alive for efficient batch processing
export OLLAMA_KEEP_ALIVE=60m
```

### Roadmap implementace

| FÃ¡ze | Funkce | Priorita | ZÃ¡vislosti |
|------|--------|----------|------------|
| **Phase 1** | StoryOutline + StoryBible objekty | ğŸŸ¢ VysokÃ¡ | Å½Ã¡dnÃ© |
| **Phase 2** | StoryBlock + Memory persistence | ğŸŸ¡ StÅ™ednÃ­ | Phase 1 |
| **Phase 3** | Moving-Window Engine | ğŸŸ¡ StÅ™ednÃ­ | Phase 1, 2 |
| **Phase 4** | Loop Mode v T.Script | ğŸŸ  NÃ­zkÃ¡ | Phase 1, 2, 3 |
| **Phase 5** | GPT-5.1 orchestrace | ğŸŸ  NÃ­zkÃ¡ | Phase 1-4 |
| **Phase 6** | ReviewResult interface standardizace | ğŸŸ¢ VysokÃ¡ | Å½Ã¡dnÃ© |
| **Phase 7** | Iterative Refinement Loop | ğŸŸ¡ StÅ™ednÃ­ | Phase 6 |
| **Phase 8** | VO-Friendliness review modul | ğŸŸ¡ StÅ™ednÃ­ | Phase 6 |
| **Phase 9** | Combined MW + IR pipeline | ğŸŸ  NÃ­zkÃ¡ | Phase 3, 7 |
| **Phase 10** | Local AI Review with Qwen 3:30B | ğŸŸ¢ VysokÃ¡ | Phase 6 |

---

## Related Documentation

- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System architecture overview
- **[T Module README](../../T/README.md)** - Text generation pipeline
- **[SEO Keywords Module](../../T/Publishing/SEO/Keywords/README.MD)** - SEO Keywords module documentation

## Version History

### 1.0.0 (2025-12-05)
- Initial documentation
- Ollama installation instructions
- Qwen2.5-14B model setup
- Python integration examples
