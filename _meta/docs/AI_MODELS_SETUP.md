# AI Models Setup Guide

**Document Type**: Setup Guide  
**Scope**: Project-wide  
**Last Updated**: 2025-12-05

## Overview

PrismQ uses local LLM models through Ollama for AI-powered content generation and SEO metadata optimization. This guide covers how to set up Ollama and configure AI models for use with PrismQ.

## Prerequisites

- Windows, macOS, or Linux operating system
- At least 16GB RAM (32GB recommended for larger models)
- GPU with sufficient VRAM for model inference (optional but recommended)
- Stable internet connection for model downloads

## Step 1 â€“ Install Ollama

Download and install Ollama from the official website:

**Download Link**: https://ollama.com/download

### Windows Installation
1. Download the Windows installer from the link above
2. Run the installer and follow the prompts
3. Ollama will be available as a system service

### macOS Installation
1. Download the macOS app from the link above
2. Drag Ollama to your Applications folder
3. Launch Ollama from Applications

### Linux Installation
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

## Step 2 â€“ Pull the Qwen2.5-14B Model

Ollama has an official Qwen2.5 entry in its library, including a 14B variant. This is the recommended model for PrismQ content generation tasks.

In your terminal, run:

```bash
ollama pull qwen2.5:14b
```

This will download approximately 9GB of model weights. The download time depends on your internet connection speed.

### Alternative Qwen2.5 Models

Depending on your hardware capabilities, you can use different Qwen2.5 variants:

| Model | Size | VRAM Required | Use Case |
|-------|------|---------------|----------|
| `qwen2.5:7b` | ~4.5GB | 8GB | Lighter weight, faster inference |
| `qwen2.5:14b` | ~9GB | 16GB | **Recommended** - Best balance of quality and speed |
| `qwen2.5:32b` | ~20GB | 24GB+ | Highest quality, requires high-end GPU |

### Other Supported Models

PrismQ also supports other Ollama models. For SEO-specific tasks, the Keywords module uses:

| Model | Size | VRAM Required | Use Case |
|-------|------|---------------|----------|
| `llama3.1:70b-q4_K_M` | ~40GB | 48GB+ | SEO metadata generation (Keywords module default) |

To pull this model:
```bash
ollama pull llama3.1:70b-q4_K_M
```

## Model Comparison for High-End Systems (RTX 5090)

For users with an NVIDIA RTX 5090 (32GB VRAM), you have access to the most powerful local AI models. Here's a comprehensive comparison for story writing and content generation:

### Recommended Models for RTX 5090

| Model | Parameters | VRAM Usage | Story Quality | Speed | Best For |
|-------|------------|------------|---------------|-------|----------|
| **Qwen2.5:32b** | 32B | ~20GB | â­â­â­â­â­ | Medium | **Best overall for creative writing** |
| **Qwen2.5:14b** | 14B | ~9GB | â­â­â­â­ | Fast | Balanced quality and speed |
| **Llama3.1:70b-q4** | 70B | ~40GB | â­â­â­â­â­ | Slow | Highest quality, requires quantization |
| **Llama3.3:70b** | 70B | ~40GB | â­â­â­â­â­ | Slow | Latest Llama, improved reasoning |
| **Mistral-Large** | 123B | ~32GB | â­â­â­â­â­ | Slow | Complex narratives |
| **DeepSeek-V2** | 236B | ~32GB | â­â­â­â­ | Medium | Long-form content |

### Llama 3.1 405B vs Llama 3.3 70B

| Aspect | Llama 3.1 â€” 405B | Llama 3.3 â€” 70B |
|--------|------------------|-----------------|
| **Parameters** | 405B | 70B |
| **VRAM Required** | ~200GB+ (requires multi-GPU or cloud) | ~40GB (Q4 quantized) |
| **Context Length** | 128K tokens | 128K tokens |
| **Quality** | â­â­â­â­â­ (best-in-class) | â­â­â­â­â­ (excellent) |
| **Speed** | Very Slow | Medium |
| **Local RTX 5090** | âŒ Too large | âœ… With quantization |
| **Ollama Support** | âŒ Cloud/API only | âœ… `ollama pull llama3.3:70b` |

### Understanding Quantization

**Co je kvantizace?** Kvantizace je technika komprese modelu, kterÃ¡ sniÅ¾uje pÅ™esnost vah (napÅ™. z 16-bit na 4-bit). To vÃ½raznÄ› zmenÅ¡Ã­ velikost modelu a VRAM poÅ¾adavky.

#### KompletnÃ­ pÅ™ehled kvantizaÄnÃ­ch variant

| Kvantizace | Kvalita vs OriginÃ¡l | VRAM (70B model) | Rychlost | DoporuÄenÃ­ |
|------------|---------------------|------------------|----------|------------|
| **FP16** (bez kvantizace) | 100% | ~140GB | NejpomalejÅ¡Ã­ | âŒ PÅ™Ã­liÅ¡ velkÃ© pro RTX 5090 |
| **Q8_0** (8-bit) | ~99.5% | ~70GB | PomalÃ¡ | âŒ PÅ™Ã­liÅ¡ velkÃ© pro RTX 5090 |
| **Q6_K** (6-bit) | ~98.5% | ~54GB | StÅ™ednÃ­ | âš ï¸ Na hranici, mÅ¯Å¾e fungovat s offloadingem |
| **Q5_K_M** (5-bit) | ~97% | ~47GB | RychlÃ¡ | âœ… DobrÃ¡ volba pro kvalitu |
| **Q5_K_S** (5-bit small) | ~96% | ~45GB | RychlÃ¡ | âœ… DobrÃ¡ alternativa |
| **Q4_K_M** (4-bit medium) | ~95% | ~40GB | Velmi rychlÃ¡ | âœ… **DOPORUÄŒENO pro RTX 5090** |
| **Q4_K_S** (4-bit small) | ~94% | ~38GB | Velmi rychlÃ¡ | âœ… NejrychlejÅ¡Ã­ kvalitnÃ­ varianta |
| **Q3_K_M** (3-bit) | ~90% | ~33GB | ExtrÃ©mnÄ› rychlÃ¡ | âš ï¸ ZnatelnÃ¡ ztrÃ¡ta kvality |
| **IQ4_XS** (4-bit i-quant) | ~94.5% | ~36GB | Velmi rychlÃ¡ | âœ… ModernÃ­ alternativa k Q4 |

#### ðŸ† FinÃ¡lnÃ­ doporuÄenÃ­ pro RTX 5090 (32GB VRAM)

**Pro maximÃ¡lnÃ­ kvalitu:** `Q4_K_M` nebo `Q5_K_S`
- Q4_K_M nabÃ­zÃ­ nejlepÅ¡Ã­ pomÄ›r kvalita/VRAM pro 32GB karty
- RozdÃ­l mezi Q4_K_M a Q6_K je v praxi tÃ©mÄ›Å™ nepostÅ™ehnutelnÃ½ pro kreativnÃ­ psanÃ­
- Q6_K je pÅ™Ã­liÅ¡ velkÃ½ pro RTX 5090 bez CPU offloadingu

```bash
# ðŸ† NEJLEPÅ Ã VOLBA pro RTX 5090 32GB - Llama 3.3 70B Q4_K_M
ollama pull llama3.3:70b-q4_K_M

# Alternativa pro o nÄ›co vyÅ¡Å¡Ã­ kvalitu (pomalejÅ¡Ã­)
ollama pull llama3.3:70b-q5_K_S

# Pro Qwen2.5 32B (vejde se celÃ½ bez kvantizace)
ollama pull qwen2.5:32b
```

> **PoznÃ¡mka ke Q6_K:** I kdyÅ¾ Q6_K nabÃ­zÃ­ ~98.5% kvality, vyÅ¾aduje ~54GB VRAM pro 70B model. Na RTX 5090 (32GB) by musel pouÅ¾Ã­t CPU offloading, coÅ¾ dramaticky zpomalÃ­ inference. Pro vaÅ¡i sestavu doporuÄuji Q4_K_M - ztrÃ¡ta kvality je minimÃ¡lnÃ­ (~5%) a rychlost bude vÃ½raznÄ› lepÅ¡Ã­.

### Model Recommendations by PrismQ Task

| Task | Best Model | Alternative | Why |
|------|------------|-------------|-----|
| **Idea Generation** | Llama 3.3:70b | Qwen2.5:32b | Strong creative reasoning, diverse ideas |
| **Title Creation** | Qwen2.5:14b | Llama 3.3:70b | Fast, concise outputs, good for iteration |
| **Script Writing** | Qwen2.5:32b | Llama 3.1:70b-q4 | Best narrative quality, instruction following |
| **Review/Editing** | Llama 3.3:70b | Llama 3.1:70b-q4 | Superior analytical and reasoning capabilities |
| **SEO Metadata** | Llama 3.1:70b-q4 | Qwen2.5:14b | Consistent, structured outputs |

#### Task-Specific Configuration:

```python
from T.Publishing.SEO.Keywords import AIConfig

# Idea Generation - creative, diverse
idea_config = AIConfig(
    model="llama3.3:70b",
    temperature=0.8,  # Higher for creativity
    enable_ai=True
)

# Title Creation - fast iteration
title_config = AIConfig(
    model="qwen2.5:14b",
    temperature=0.5,
    enable_ai=True
)

# Script Writing - high quality narrative
script_config = AIConfig(
    model="qwen2.5:32b",
    temperature=0.7,
    max_tokens=4000,
    enable_ai=True
)

# Review/Editing - analytical
review_config = AIConfig(
    model="llama3.3:70b",
    temperature=0.3,  # Lower for consistent analysis
    enable_ai=True
)
```

#### When to Use Llama 3.1 405B:
- Cloud/API access (Together AI, Fireworks, Groq)
- Maximum quality requirements
- Complex reasoning tasks
- Enterprise production workflows

#### When to Use Llama 3.3 70B:
- **Recommended for local RTX 5090** âœ…
- Best balance of quality and local inference
- Reviews and analytical tasks
- Idea generation with strong reasoning

```bash
# Install Llama 3.3 70B for local use
ollama pull llama3.3:70b-q4_K_M
```

### MPT-7B-StoryWriter (HuggingFace)

The [MPT-7B-StoryWriter](https://huggingface.co/mosaicml/mpt-7b-storywriter) is a specialized model fine-tuned specifically for story writing with an impressive 65K context length.

| Aspect | MPT-7B-StoryWriter | Qwen2.5:32b |
|--------|-------------------|-------------|
| **Parameters** | 7B | 32B |
| **VRAM Required** | ~8GB | ~20GB |
| **Context Length** | 65,536 tokens | 32,768 tokens |
| **Story Quality** | â­â­â­â­ (specialized) | â­â­â­â­â­ (general) |
| **Inference Speed** | Very Fast | Medium |
| **Ollama Support** | âŒ (requires manual setup) | âœ… Native |
| **Best Use Case** | Long-form novels, extended narratives | All creative content |

#### When to Choose MPT-7B-StoryWriter:
- Writing very long stories (novels, extended series)
- Need 65K context for maintaining consistency
- Prefer faster inference over raw quality
- Working with HuggingFace Transformers directly

#### When to Choose Qwen2.5:32b:
- General creative writing (stories, scripts, dialogue)
- Need better instruction following
- Prefer easy Ollama integration
- Want highest quality output

### ðŸ“š Modely optimalizovanÃ© pro tvorbu pÅ™Ã­bÄ›hÅ¯

Pro kreativnÃ­ psanÃ­ a tvorbu pÅ™Ã­bÄ›hÅ¯ existujÃ­ specializovanÃ© modely s lepÅ¡Ã­m vÃ½konem neÅ¾ obecnÃ© LLM:

#### ðŸ§ª ReÃ¡lnÃ© testy modelÅ¯ (English YA Fiction)

Na zÃ¡kladÄ› internÃ­ho testovÃ¡nÃ­ PrismQ s anglickÃ½mi pÅ™Ã­bÄ›hy pro US/CA publikum:

| Model | Test pÅ™Ã­bÄ›h | SkÃ³re | Struktura | Dialog | Postavy | NapÄ›tÃ­ | Konzistence | ÄŒitelnost | PoznÃ¡mka |
|-------|-------------|-------|-----------|--------|---------|--------|-------------|-----------|----------|
| **qwen2.5:32b (EN)** | The Lighthouse Keeper's Secret | **7.8/10** | 8 | 9 | 6.5 | 8 | 7 | 7.5 | 7 | ðŸ† PÅ™ekvapivÄ› ÄistÃ©, soudrÅ¾nÃ©, ÄtivÃ© â€” mnohem lepÅ¡Ã­ neÅ¾ CZ |
| **mistral-nemo:12b (EN)** | The Whispering Grove | **7.5/10** | 8 | 9 | 6 | 7.5 | 6 | 7 | 6 | AngliÄtina vÃ½raznÄ› zvedÃ¡ kvalitu, dobrÃ© YA-fantasy |

> **KlÃ­ÄovÃ© zjiÅ¡tÄ›nÃ­:** AnglickÃ¡ verze vÃ½raznÄ› pÅ™evyÅ¡uje Äeskou u obou modelÅ¯. Pro US/CA publikum doporuÄujeme vÅ¾dy generovat v angliÄtinÄ›.

#### ðŸ† FinÃ¡lnÃ­ doporuÄenÃ­ pro English YA Fiction

Na zÃ¡kladÄ› testÅ¯ doporuÄujeme pro **americkÃ©/kanadskÃ© teen publikum**:

| Priorita | Model | SkÃ³re | NejlepÅ¡Ã­ pro |
|----------|-------|-------|--------------|
| **#1** | **Qwen2.5:32b** | 7.8/10 | Family Drama, Teen Drama, Mystery |
| **#2** | **Mistral-Nemo:12b** | 7.5/10 | YA Fantasy, Slice of Life, Reddit Stories |

```bash
# PrimÃ¡rnÃ­ model pro EN YA content
ollama pull qwen2.5:32b

# SekundÃ¡rnÃ­/rychlejÅ¡Ã­ model
ollama pull mistral-nemo:12b
```

#### ðŸ” SrovnÃ¡nÃ­ 32B modelÅ¯ pro kreativnÃ­ psanÃ­

| Model | Fine-tuning | Kvalita prÃ³zy | Kontext | AngliÄtina | Ollama | Benchmarks |
|-------|-------------|---------------|---------|------------|--------|------------|
| **Qwen2.5:32b-Instruct** | General | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 83.5 |
| **Yi-1.5-34B-Chat** | Chat/Creative | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 81.2 |
| **DeepSeek-V2-Lite (27B)** | General | â­â­â­â­ | 128K | DobrÃ¡ | âœ… | MMLU: 79.8 |
| **Mixtral-8x7B (47B MoE)** | Instruct | â­â­â­â­â­ | 32K | VÃ½bornÃ¡ | âœ… | MMLU: 81.1 |
| **Command-R (35B)** | RAG/Chat | â­â­â­â­ | 128K | VÃ½bornÃ¡ | âœ… | MMLU: 78.5 |

#### ðŸ“– Fine-tuned modely pro kreativnÃ­ psanÃ­ (32B tÅ™Ã­da)

| Model | Specializace | Kvalita | VRAM | Zdroj |
|-------|--------------|---------|------|-------|
| **Nous-Hermes-2-Yi-34B** | Creative writing, RP | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Airoboros-34B** | Creative, storytelling | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Dolphin-2.6-Yi-34B** | Uncensored creative | â­â­â­â­â­ | ~22GB | HuggingFace |
| **Goliath-120B** (merged) | Premium creative | â­â­â­â­â­ | ~70GB | HuggingFace |
| **Chronos-Hermes-34B** | Long-form fiction | â­â­â­â­â­ | ~22GB | HuggingFace |
| **StellarBright-Qwen2.5-32B** | Creative writing | â­â­â­â­â­ | ~20GB | HuggingFace |

> **PoznÃ¡mka:** Fine-tuned modely pro kreativnÃ­ psanÃ­ Äasto pÅ™ekonÃ¡vajÃ­ vÄ›tÅ¡Ã­ obecnÃ© modely v kvalitÄ› narativu.

#### ðŸŽ¯ DoporuÄenÃ­ pro cÃ­lovÃ© publikum: Teen/Young Adult (10-20, US Å¾eny)

Pro americkÃ© a kanadskÃ© anglicky mluvÃ­cÃ­ publikum (pÅ™edevÅ¡Ã­m mladÃ© Å¾eny 10-20 let):

| Å½Ã¡nr | ðŸ† DoporuÄenÃ½ model | Alternativa | ProÄ |
|------|---------------------|-------------|------|
| **Reddit Stories** | Nous-Hermes-2-Yi-34B | Qwen2.5:32b | AutentickÃ½ Reddit styl, relatable |
| **Family Drama** | Qwen2.5:32b | Airoboros-34B | EmocionÃ¡lnÃ­ hloubka, realistickÃ© dialogy |
| **Teen Drama** | Dolphin-2.6-Yi-34B | Nous-Hermes-2-Yi-34B | Teen hlas, modernÃ­ slang |
| **Teen Stories** | Nous-Hermes-2-Yi-34B | Mistral-Nemo | YA narativ, engagement |
| **Romance (YA)** | Chronos-Hermes-34B | Qwen2.5:32b | EmotivnÃ­, clean romance |
| **Thriller/Mystery** | Qwen2.5:32b | Command-R | NapÄ›tÃ­, twist endings |
| **AITA/Confession** | Nous-Hermes-2-Yi-34B | Dolphin-2.6-Yi-34B | AutentickÃ½ POV |
| **Slice of Life** | Mistral-Nemo | Qwen2.5:32b | KaÅ¾dodennÃ­ situace, relatability |

#### ðŸ† TOP 3 pro Teen/YA obsah na RTX 5090

**1. Nous-Hermes-2-Yi-34B** - NejlepÅ¡Ã­ pro Reddit/Teen stories
```bash
ollama pull nous-hermes2:yi-34b-q4_K_M
```
- SpecializovanÃ½ na creative writing a roleplay
- PÅ™irozenÃ½ teen dialog a POV
- VÃ½bornÃ½ pro AITA, relationship drama, confession stories
- ~22GB VRAM (Q4)

**2. Qwen2.5:32b** - UniverzÃ¡lnÃ­ vysokÃ¡ kvalita
```bash
ollama pull qwen2.5:32b
```
- NejlepÅ¡Ã­ balance kvality a rychlosti
- VÃ½bornÃ¡ angliÄtina, emotivnÃ­ prÃ³za
- IdeÃ¡lnÃ­ pro family drama, romance
- ~20GB VRAM

**3. Dolphin-2.6-Yi-34B** - Pro autentickÃ½ teen hlas
```bash
ollama pull dolphin2.6:yi-34b-q4_K_M
```
- Uncensored, pÅ™irozenÃ© dialogy
- ModernÃ­ slang a teen expressions
- VhodnÃ½ pro edgier teen drama
- ~22GB VRAM (Q4)

#### ðŸ“Š Statistiky a benchmarky 32B modelÅ¯ (creative writing)

Na zÃ¡kladÄ› komunitnÃ­ch testÅ¯ a r/LocalLLaMA:

| Model | Reddit Stories | Dialogy | Emotivnost | Konzistence | CelkovÄ› |
|-------|----------------|---------|------------|-------------|---------|
| Nous-Hermes-2-Yi-34B | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­ | **#1** |
| Qwen2.5:32b | â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ | **#2** |
| Airoboros-34B | â­â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | **#3** |
| Yi-34B-Chat | â­â­â­â­ | â­â­â­â­ | â­â­â­â­ | â­â­â­â­â­ | **#4** |

#### Konfigurace pro Teen/YA content

```python
from T.Publishing.SEO.Keywords import AIConfig

# Reddit Stories / AITA style
reddit_config = AIConfig(
    model="nous-hermes2:yi-34b-q4_K_M",
    temperature=0.85,           # VyÅ¡Å¡Ã­ pro autenticitu
    max_tokens=3000,
    enable_ai=True
)

# Teen Drama / Family Drama
drama_config = AIConfig(
    model="qwen2.5:32b",
    temperature=0.75,
    max_tokens=4000,
    enable_ai=True
)

# Teen Stories (YA fiction)
teen_stories_config = AIConfig(
    model="nous-hermes2:yi-34b-q4_K_M",
    temperature=0.8,
    max_tokens=5000,           # DelÅ¡Ã­ kapitoly
    enable_ai=True
)
```

#### Instalace modelÅ¯ pro Teen/YA content

```bash
# KompletnÃ­ sada pro Teen/YA publikum na RTX 5090

# 1. PrimÃ¡rnÃ­ pro Reddit stories a teen drama
ollama pull nous-hermes2:yi-34b-q4_K_M

# 2. UniverzÃ¡lnÃ­ vysokÃ¡ kvalita
ollama pull qwen2.5:32b

# 3. Pro edgier teen content
ollama pull dolphin2.6:yi-34b-q4_K_M

# 4. Pro dlouhÃ© sÃ©rie
ollama pull yi:34b-chat-q4_K_M
```

> **Tip pro US teen publikum:** PouÅ¾Ã­vejte `temperature=0.8-0.9` pro autentiÄtÄ›jÅ¡Ã­ dialogy. Teen content vyÅ¾aduje souÄasnÃ½ slang, pop culture reference a relatable situace.

#### SrovnÃ¡nÃ­ modelÅ¯ pro psanÃ­ pÅ™Ã­bÄ›hÅ¯

| Model | Parametry | VRAM | Kvalita pÅ™Ã­bÄ›hÅ¯ | Kontext | Ollama | DoporuÄenÃ­ |
|-------|-----------|------|-----------------|---------|--------|------------|
| **Mistral-Nemo-Instruct-2407** | 12B | ~8GB | â­â­â­â­â­ | 128K | âœ… | ðŸ† **NEJLEPÅ Ã pro pÅ™Ã­bÄ›hy** |
| **Qwen2.5:32b** | 32B | ~20GB | â­â­â­â­â­ | 32K | âœ… | VÃ½bornÃ¡ kvalita, versatilnÃ­ |
| **Llama-3.1-Storm-8B** | 8B | ~6GB | â­â­â­â­ | 8K | âœ… | KreativnÃ­, rychlÃ½ |
| **Nous-Hermes-2-Mixtral** | 47B | ~28GB | â­â­â­â­â­ | 32K | âœ… | NejlepÅ¡Ã­ MoE pro pÅ™Ã­bÄ›hy |
| **Yi-34B-Chat** | 34B | ~22GB | â­â­â­â­â­ | 200K | âœ… | ExtrÃ©mnÃ­ kontext |
| **DeepSeek-Coder-V2-Lite** | 16B | ~10GB | â­â­â­â­ | 128K | âœ… | DobrÃ½ pro dialogy |
| **MPT-7B-StoryWriter** | 7B | ~8GB | â­â­â­â­ | 65K | âŒ | SpecializovanÃ½ na romÃ¡ny |
| **Fimbulvetr-11B** | 11B | ~8GB | â­â­â­â­â­ | 8K | âš ï¸ | VÃ½jimeÄnÃ½ pro RP/fiction |

#### ðŸ† Top 3 doporuÄenÃ© modely pro pÅ™Ã­bÄ›hy na RTX 5090

**1. Mistral-Nemo-Instruct-2407** - NejlepÅ¡Ã­ volba
```bash
ollama pull mistral-nemo:12b
```
- 128K tokenÅ¯ kontextu (perfektnÃ­ pro dlouhÃ© pÅ™Ã­bÄ›hy)
- VÃ½jimeÄnÃ¡ kreativita a koherence
- OptimalizovÃ¡no pro narativnÃ­ Ãºlohy
- Vejde se do 32GB VRAM bez kvantizace

**2. Nous-Hermes-2-Mixtral-8x7B** - Premium kvalita
```bash
ollama pull nous-hermes2-mixtral:8x7b-q4_K_M
```
- MoE architektura (efektivnÃ­ vyuÅ¾itÃ­ parametrÅ¯)
- Å piÄkovÃ¡ kvalita prÃ³zy
- VyÅ¾aduje ~28GB VRAM

**3. Yi-34B-Chat** - Pro extrÃ©mnÄ› dlouhÃ© pÅ™Ã­bÄ›hy
```bash
ollama pull yi:34b-chat-q4_K_M
```
- 200K tokenÅ¯ kontextu (nejdelÅ¡Ã­)
- IdeÃ¡lnÃ­ pro romÃ¡ny a sÃ©rie
- VyÅ¾aduje ~22GB VRAM (Q4)

#### SpecializovanÃ© modely pro rÅ¯znÃ© Å¾Ã¡nry

| Å½Ã¡nr | DoporuÄenÃ½ model | Alternativa |
|------|------------------|-------------|
| **Horror/Dark Fantasy** | Mistral-Nemo | Fimbulvetr-11B |
| **Romantika** | Qwen2.5:32b | Yi-34B-Chat |
| **Sci-Fi** | Nous-Hermes-2-Mixtral | DeepSeek-V2 |
| **Detektivky** | Llama 3.3:70b | Mistral-Nemo |
| **DÄ›tskÃ© pÅ™Ã­bÄ›hy** | Qwen2.5:14b | Mistral-Nemo |
| **EpickÃ¡ fantasy** | Yi-34B-Chat | Nous-Hermes-2 |
| **KrÃ¡tkÃ© povÃ­dky** | Mistral-Nemo | Qwen2.5:32b |
| **Dialogy/ScÃ©nÃ¡Å™e** | Llama 3.3:70b | Qwen2.5:32b |

#### Instalace nejlepÅ¡Ã­ch modelÅ¯ pro pÅ™Ã­bÄ›hy

```bash
# KompletnÃ­ sada pro profesionÃ¡lnÃ­ tvorbu pÅ™Ã­bÄ›hÅ¯ na RTX 5090

# 1. PrimÃ¡rnÃ­ model pro pÅ™Ã­bÄ›hy (doporuÄeno)
ollama pull mistral-nemo:12b

# 2. Pro dlouhÃ© romÃ¡ny
ollama pull yi:34b-chat-q4_K_M

# 3. Pro premium kvalitu prÃ³zy
ollama pull nous-hermes2-mixtral:8x7b-q4_K_M

# 4. UniverzÃ¡lnÃ­ zÃ¡loha
ollama pull qwen2.5:32b
```

#### Konfigurace pro tvorbu pÅ™Ã­bÄ›hÅ¯

```python
from T.Publishing.SEO.Keywords import AIConfig

# OptimÃ¡lnÃ­ konfigurace pro kreativnÃ­ psanÃ­ pÅ™Ã­bÄ›hÅ¯
story_writing_config = AIConfig(
    model="mistral-nemo:12b",  # NejlepÅ¡Ã­ pro pÅ™Ã­bÄ›hy
    api_base="http://localhost:11434",
    temperature=0.8,           # VyÅ¡Å¡Ã­ pro kreativitu
    max_tokens=4000,           # DlouhÃ© kapitoly
    enable_ai=True
)

# Pro velmi dlouhÃ© pÅ™Ã­bÄ›hy (romÃ¡ny)
novel_config = AIConfig(
    model="yi:34b-chat-q4_K_M",
    temperature=0.7,
    max_tokens=8000,           # CelÃ© kapitoly
    enable_ai=True
)

# Pro dialogy a scÃ©nÃ¡Å™e
dialogue_config = AIConfig(
    model="llama3.3:70b-q4_K_M",
    temperature=0.6,           # KonzistentnÄ›jÅ¡Ã­ dialogy
    max_tokens=2000,
    enable_ai=True
)
```

> **Tip pro tvorbu pÅ™Ã­bÄ›hÅ¯:** PouÅ¾Ã­vejte vyÅ¡Å¡Ã­ `temperature` (0.7-0.9) pro kreativnÄ›jÅ¡Ã­ vÃ½stup. Pro konzistentnÃ­ postavy a zÃ¡pletky udrÅ¾ujte kontext a pouÅ¾Ã­vejte modely s dlouhÃ½m kontextovÃ½m oknem (Yi-34B, Mistral-Nemo).

### RTX 5090 Optimal Configuration

For RTX 5090 with 32GB VRAM, we recommend:

```bash
# Best quality for story writing
ollama pull qwen2.5:32b

# Alternative for SEO and metadata
ollama pull llama3.1:70b-q4_K_M
```

**PrismQ Configuration for RTX 5090:**

```python
from T.Publishing.SEO.Keywords import AIConfig

# High-quality story generation config
story_config = AIConfig(
    model="qwen2.5:32b",
    api_base="http://localhost:11434",
    temperature=0.7,  # Higher for creative writing
    max_tokens=2000,
    enable_ai=True
)

# SEO metadata config
seo_config = AIConfig(
    model="llama3.1:70b-q4_K_M",
    api_base="http://localhost:11434",
    temperature=0.3,  # Lower for consistent SEO output
    enable_ai=True
)
```

### Optimalizace naÄÃ­tÃ¡nÃ­ modelu (Model Loading Optimization)

Pro zamezenÃ­ opakovanÃ©ho naÄÃ­tÃ¡nÃ­ modelu do VRAM bÄ›hem bÄ›hu PrismQ:

#### Ollama Keep-Alive nastavenÃ­

Ollama standardnÄ› udrÅ¾uje model v pamÄ›ti 5 minut po poslednÃ­m dotazu. Pro delÅ¡Ã­ workflow:

```bash
# Nastavte OLLAMA_KEEP_ALIVE na delÅ¡Ã­ dobu (napÅ™. 60 minut)
export OLLAMA_KEEP_ALIVE=60m

# Nebo permanentnÄ› v .bashrc / .zshrc
echo 'export OLLAMA_KEEP_ALIVE=60m' >> ~/.bashrc
```

**Windows (PowerShell):**
```powershell
# Nastavte promÄ›nnou prostÅ™edÃ­
$env:OLLAMA_KEEP_ALIVE = "60m"

# Nebo permanentnÄ›
[System.Environment]::SetEnvironmentVariable("OLLAMA_KEEP_ALIVE", "60m", "User")
```

#### PrismQ Model Manager (doporuÄenÃ½ pÅ™Ã­stup)

Pro optimÃ¡lnÃ­ vÃ½kon pouÅ¾ijte jednotnÃ½ model pro celÃ½ workflow:

```python
import ollama

class PrismQModelManager:
    """SprÃ¡vce modelu pro efektivnÃ­ vyuÅ¾itÃ­ VRAM."""
    
    _instance = None
    _current_model = None
    
    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance
    
    def ensure_model_loaded(self, model_name: str):
        """NaÄte model pouze pokud jeÅ¡tÄ› nenÃ­ v pamÄ›ti."""
        if self._current_model != model_name:
            # Warmup dotaz pro naÄtenÃ­ modelu do VRAM
            ollama.chat(
                model=model_name,
                messages=[{"role": "user", "content": "Hello"}],
                options={"num_predict": 1}
            )
            self._current_model = model_name
            print(f"Model {model_name} naÄten do VRAM")
        return self._current_model

# PouÅ¾itÃ­ na zaÄÃ¡tku workflow
manager = PrismQModelManager.get_instance()
manager.ensure_model_loaded("qwen2.5:32b")

# VÅ¡echny nÃ¡sledujÃ­cÃ­ dotazy pouÅ¾ijÃ­ jiÅ¾ naÄtenÃ½ model
```

#### DoporuÄenÃ¡ strategie pro celÃ½ PrismQ workflow

| FÃ¡ze | Model | DÅ¯vod |
|------|-------|-------|
| **Idea â†’ Title â†’ Script â†’ Review** | `qwen2.5:32b` | Jeden model pro celÃ½ workflow, bez pÅ™epÃ­nÃ¡nÃ­ |
| **SEO Metadata** (volitelnÄ›) | PÅ™epnout na `llama3.3:70b-q4_K_M` | Pouze pokud je nutnÃ¡ lepÅ¡Ã­ SEO kvalita |

> **Tip:** Pro maximÃ¡lnÃ­ efektivitu pouÅ¾Ã­vejte jeden model pro celÃ½ bÄ›h. PÅ™epÃ­nÃ¡nÃ­ mezi modely vyÅ¾aduje uvolnÄ›nÃ­ a naÄtenÃ­ ~20-40GB dat, coÅ¾ trvÃ¡ 10-30 sekund.

### OptimÃ¡lnÃ­ konfigurace pro Ryzen 9 9900X3D + RTX 5090

Pro vÃ¡Å¡ konkrÃ©tnÃ­ hardware (AMD Ryzen 9 9900X3D + RTX 5090 32GB):

| Parametr | DoporuÄenÃ¡ hodnota | DÅ¯vod |
|----------|-------------------|-------|
| **Model** | `qwen2.5:32b` nebo `llama3.3:70b-q4_K_M` | PlnÄ› vyuÅ¾ije 32GB VRAM |
| **Kvantizace (70B)** | Q4_K_M | OptimÃ¡lnÃ­ pro 32GB VRAM |
| **Context Length** | 8192-16384 | VyuÅ¾ije 3D V-Cache pro KV cache |
| **Batch Size** | 1 | StandardnÃ­ pro generovÃ¡nÃ­ |
| **GPU Layers** | All (auto) | CelÃ½ model na GPU |

```bash
# OptimÃ¡lnÃ­ Ollama konfigurace pro Ryzen 9 9900X3D + RTX 5090
export OLLAMA_NUM_PARALLEL=1          # Jeden request najednou
export OLLAMA_KEEP_ALIVE=60m          # Model zÅ¯stane v pamÄ›ti
export OLLAMA_MAX_LOADED_MODELS=1     # Jeden model najednou (Å¡etÅ™Ã­ VRAM)

# SpusÅ¥te Ollama
ollama serve
```

**VyuÅ¾itÃ­ 3D V-Cache (141MB):**
Ryzen 9 9900X3D mÃ¡ masivnÃ­ L3 cache, kterÃ¡ pomÃ¡hÃ¡ s:
- RychlejÅ¡Ã­m tokenizaÄnÃ­m pre/post-processingem
- EfektivnÄ›jÅ¡Ã­m CPU offloadingem (pokud potÅ™ebnÃ½)
- NiÅ¾Å¡Ã­ latencÃ­ pÅ™i komunikaci s GPU

### Using MPT-7B-StoryWriter with HuggingFace

If you prefer the specialized MPT-7B-StoryWriter model:

```bash
pip install transformers torch accelerate
```

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "mosaicml/mpt-7b-storywriter"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto",
    trust_remote_code=True
)

prompt = "Write the opening chapter of a dark horror story set in a small Czech town:"
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")

outputs = model.generate(
    **inputs,
    max_new_tokens=1000,
    temperature=0.7,
    do_sample=True
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

> **Note**: MPT-7B-StoryWriter requires manual setup with HuggingFace Transformers and is not directly supported by Ollama. For easier integration with PrismQ, we recommend Qwen2.5:32b.

## Step 3 â€“ Test the Model

After the download finishes, verify the installation:

```bash
ollama run qwen2.5:14b
```

You'll enter an interactive prompt. Try a test query:

```
Write a dark, emotional horror story opening set in a small Czech town at night.
```

If it responds with a story, Qwen2.5-14B is correctly installed. Type `/bye` to exit.

## Step 4 â€“ Python Integration

### Install the Ollama Python Package

```bash
pip install ollama
```

### Minimal Python Script

Create a file named `story_test.py`:

```python
import ollama

prompt = "Write the first 500 words of a psychological horror story told in first person."

response = ollama.chat(
    model="qwen2.5:14b",
    messages=[
        {"role": "user", "content": prompt}
    ]
)

print(response["message"]["content"])
```

Run it:

```bash
python story_test.py
```

This directly uses your local Qwen2.5-14B via Ollama's HTTP API on `localhost:11434`.

## PrismQ Integration

### Using AI in SEO Metadata Generation

PrismQ's SEO Keywords module already supports Ollama integration. Example usage:

```python
from T.Publishing.SEO.Keywords import process_content_seo, AIConfig

# Configure to use Qwen2.5-14B
config = AIConfig(
    model="qwen2.5:14b",
    api_base="http://localhost:11434",
    temperature=0.3,
    enable_ai=True
)

result = process_content_seo(
    title="Your Content Title",
    script="Your content script...",
    use_ai=True,
    ai_config=config,
    brand_name="Your Brand"
)

print(result['meta_description'])
print(result['title_tag'])
```

### Default AI Configuration

The default configuration in PrismQ varies by module:
- **SEO Keywords Module**: Uses `llama3.1:70b-q4_K_M` (optimized for SEO tasks)
- **General Content Generation**: Recommended `qwen2.5:14b` (best for creative writing)

Common settings:
- **API Base**: `http://localhost:11434`
- **Temperature**: 0.3 (lower for more consistent output)

You can customize these settings using the `AIConfig` class to match your model choice.

## Troubleshooting

### Ollama Not Running

If you see errors about Ollama being unavailable:

1. **Windows**: Check if Ollama is running in the system tray
2. **macOS**: Launch Ollama from Applications
3. **Linux**: Start the service with `ollama serve`

### Model Not Found

If a model is not found, pull it first:

```bash
ollama pull qwen2.5:14b
```

### Out of Memory Errors

If you encounter memory errors:
- Use a smaller model variant (e.g., `qwen2.5:7b` instead of `qwen2.5:14b`)
- Close other GPU-intensive applications
- Increase system swap space

### Slow Response Times

For faster inference:
- Ensure you're using GPU acceleration (NVIDIA CUDA or Apple Metal)
- Consider using quantized model variants
- Reduce `max_tokens` in the configuration

## API Reference

Ollama exposes a REST API at `http://localhost:11434`. Key endpoints:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/api/generate` | POST | Generate text completion |
| `/api/chat` | POST | Chat-style completion |
| `/api/tags` | GET | List available models |
| `/api/pull` | POST | Download a model |

### Example API Call

```python
import requests

response = requests.post(
    "http://localhost:11434/api/generate",
    json={
        "model": "qwen2.5:14b",
        "prompt": "Write a short story about...",
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 500
        }
    },
    timeout=30
)

result = response.json()
print(result["response"])
```

## Related Documentation

- **[ARCHITECTURE.md](./ARCHITECTURE.md)** - System architecture overview
- **[T Module README](../../T/README.md)** - Text generation pipeline
- **[SEO Keywords Module](../../T/Publishing/SEO/Keywords/README.MD)** - SEO Keywords module documentation

## Version History

### 1.0.0 (2025-12-05)
- Initial documentation
- Ollama installation instructions
- Qwen2.5-14B model setup
- Python integration examples
